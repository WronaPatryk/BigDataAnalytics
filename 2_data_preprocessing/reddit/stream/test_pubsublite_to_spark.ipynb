{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b404fabc",
   "metadata": {},
   "source": [
    "## Loading Pub/Sub Lite to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1fcd2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructType, StructField, IntegerType, LongType, FloatType, DateType\n",
    "from pyspark.sql.functions import from_json, col\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48c577bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_number = 1072423212419\n",
    "location = \"europe-central2\"\n",
    "lite_subscription_id = \"bda-reddit-sub-lite\"\n",
    "\n",
    "key_file = open(\"/home/bda_crypto_busters/repos/BigDataAnalytics/2_data_preprocessing/crypto/stream/crypto-busting-375023-6722d6967eca.json\", \"rb\")\n",
    "key = base64.b64encode(key_file.read())\n",
    "key = key.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "326ad5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.jars\", \"/home/bda_crypto_busters/repos/BigDataAnalytics/2_data_preprocessing/crypto/stream/pubsublite-spark-sql-streaming-LATEST-with-dependencies.jar\")\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "    .appName(\"Read Pub/Sub Lite Stream\")\n",
    "    .master(\"yarn\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85ab121d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n",
      "23/01/22 19:06:06 WARN org.apache.hadoop.hive.ql.session.SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[result: int]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"add jar gs://spark-lib/pubsublite/pubsublite-spark-sql-streaming-LATEST-with-dependencies.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "893ab3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.readStream.format(\"pubsublite\")\n",
    "    .option(\n",
    "        \"pubsublite.subscription\",\n",
    "        f\"projects/{project_number}/locations/{location}/subscriptions/{lite_subscription_id}\",\n",
    "    )\n",
    "    .option(\"gcp.credentials.key\", key)\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94856bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"data\", df.data.cast(StringType())).select(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d32f4e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = (\n",
    "#     df.writeStream.format(\"console\")\n",
    "#     .outputMode(\"append\")\n",
    "#     .trigger(processingTime=\"1 second\")\n",
    "#     .start()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cc76b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSONschema = StructType([ \n",
    "    StructField(\"id\", StringType(), False),\n",
    "    StructField(\"title\", StringType(), False),\n",
    "    StructField(\"text\", StringType(), True),\n",
    "    StructField(\"time\", FloatType(), False),\n",
    "    StructField(\"upvotes\", IntegerType(), False),\n",
    "    StructField(\"comments\", IntegerType(), False),\n",
    "    StructField(\"subreddit\", StringType(), False),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3934b4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = df.withColumn(\"JSONData\", from_json(col(\"data\"), JSONschema)).select(\"JSONData.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2530b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.withColumn(\"subreddit\", f.lower(col(\"subreddit\")))\\\n",
    ".withColumn(\"time\", sdf.time.cast(IntegerType()))\\\n",
    ".withColumn(\"time\", f.from_unixtime(col(\"time\")))\\\n",
    ".withColumn(\"year\", f.year(col(\"time\")))\\\n",
    ".withColumn(\"month\", f.month(col(\"time\")))\\\n",
    ".withColumn(\"day\", f.dayofmonth(col(\"time\")))\\\n",
    ".withColumn(\"hour\", f.hour(col(\"time\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcd776ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/22 19:06:19 WARN org.apache.spark.sql.streaming.StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-934b2785-8753-4378-a555-7e342cecac7c. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/01/22 19:06:19 WARN org.apache.spark.sql.streaming.StreamingQueryManager: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "    sdf.writeStream.format(\"console\")\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(processingTime=\"1 second\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133983db",
   "metadata": {},
   "source": [
    "## Counting words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "736f28ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_words = sdf.withColumn(\"word_count\", f.size(f.split(f.concat_ws(\" \", col(\"title\"), col(\"text\")), \"\\\\s+\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084d2806",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    sdf_words.writeStream.format(\"console\")\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(processingTime=\"1 second\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61424da",
   "metadata": {},
   "source": [
    "## Assigning sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee3edaa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/miniconda3/lib/python3.8/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.1)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "import argparse\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2f08ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/user/bda_reddit_pw/models/sentiment_model'\n",
    "day_shift = 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4c6ca4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------+--------------------+--------------------+-------------------+-------+--------+---------+----+-----+---+----+\n",
      "|     id|               title|                text|               time|upvotes|comments|subreddit|year|month|day|hour|\n",
      "+-------+--------------------+--------------------+-------------------+-------+--------+---------+----+-----+---+----+\n",
      "|10iheu5|Bill Gates WARNS ...|                    |2023-01-22 11:16:16|      1|       0|  bitcoin|2023|    1| 22|  11|\n",
      "|10ii6lj|              Noice!|                    |2023-01-22 12:03:12|      1|       0|  bitcoin|2023|    1| 22|  12|\n",
      "|10ijgv0|               DM Me|I looking for peo...|2023-01-22 13:17:52|      1|       0|  bitcoin|2023|    1| 22|  13|\n",
      "|10ijtny|            good boi|                    |2023-01-22 13:37:04|      1|       0| dogecoin|2023|    1| 22|  13|\n",
      "|10ihwtj|Calling all crypt...|                    |2023-01-22 11:48:16|      1|       0|  bitcoin|2023|    1| 22|  11|\n",
      "|10ij6y0|Banxa scam!!! Do ...|Do not use Banxa!...|2023-01-22 13:02:56|      1|       0|  bitcoin|2023|    1| 22|  13|\n",
      "|10ijzsr|Is it normal not ...|Hello everyone, I...|2023-01-22 13:45:36|      1|       1|  cardano|2023|    1| 22|  13|\n",
      "|10ik02i|Best train ride ever|                    |2023-01-22 13:45:36|      1|       0| dogecoin|2023|    1| 22|  13|\n",
      "|10ik4do|Crypto Arbitrage....|                    |2023-01-22 13:52:00|      1|       0|  bitcoin|2023|    1| 22|  13|\n",
      "|10iiru7|Where can I spend...|Going to Amsterda...|2023-01-22 12:39:28|      1|       0|  bitcoin|2023|    1| 22|  12|\n",
      "|10ik87y|Cardano issue in ...|I have been using...|2023-01-22 13:58:24|      1|       1|  cardano|2023|    1| 22|  13|\n",
      "|10ii38z|Useful vscode ext...|                    |2023-01-22 11:58:56|      1|       1|   solana|2023|    1| 22|  11|\n",
      "|10ihimo|Bitcoin network i...|Bitcoin network, ...|2023-01-22 11:22:40|      1|       0|  bitcoin|2023|    1| 22|  11|\n",
      "|10ihjc4|DCA all the way a...|By no means I'm s...|2023-01-22 11:24:48|      1|       0|  bitcoin|2023|    1| 22|  11|\n",
      "|10ihlix|Ethereum's Upcomi...|                    |2023-01-22 11:26:56|      1|       0| ethereum|2023|    1| 22|  11|\n",
      "|10ihvhd|Input Output (IOH...|                    |2023-01-22 11:44:00|      1|       1|  cardano|2023|    1| 22|  11|\n",
      "|10ii95e|Bitcoin soars pas...|                    |2023-01-22 12:07:28|      1|       0|  bitcoin|2023|    1| 22|  12|\n",
      "|10iijli|Dead cat bounce? ...|                    |2023-01-22 12:24:32|      1|       0|  bitcoin|2023|    1| 22|  12|\n",
      "|10iip2q|Sample invoice cr...|Hello guys. Does ...|2023-01-22 12:35:12|      1|       0|  bitcoin|2023|    1| 22|  12|\n",
      "|10ik0tg|Double Quarter Po...|                    |2023-01-22 13:47:44|      1|       0| dogecoin|2023|    1| 22|  13|\n",
      "+-------+--------------------+--------------------+-------------------+-------+--------+---------+----+-----+---+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/22 19:06:51 WARN org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 19402 milliseconds\n",
      "23/01/22 19:06:55 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Thread (Thread[GetFileInfo #1,5,main]) interrupted: \n",
      "java.lang.InterruptedException\n",
      "\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:510)\n",
      "\tat com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:88)\n",
      "\tat org.apache.hadoop.util.concurrent.ExecutorHelper.logThrowableFromAfterExecute(ExecutorHelper.java:48)\n",
      "\tat org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor.afterExecute(HadoopThreadPoolExecutor.java:90)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/01/22 19:06:55 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Thread (Thread[GetFileInfo #1,5,main]) interrupted: \n",
      "java.lang.InterruptedException\n",
      "\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:510)\n",
      "\tat com.google.common.util.concurrent.FluentFuture$TrustedFuture.get(FluentFuture.java:88)\n",
      "\tat org.apache.hadoop.util.concurrent.ExecutorHelper.logThrowableFromAfterExecute(ExecutorHelper.java:48)\n",
      "\tat org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor.afterExecute(HadoopThreadPoolExecutor.java:90)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "loaded_model = PipelineModel.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd44b812",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_raw = loaded_model.transform(sdf_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b998f2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:46023)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "ename": "Py4JNetworkError",
     "evalue": "An error occurred while trying to connect to the Java server (127.0.0.1:46023)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:977\u001b[0m, in \u001b[0;36mGatewayClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 977\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeque\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1115\u001b[0m, in \u001b[0;36mGatewayConnection.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1115\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mprediction_raw\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumnRenamed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubreddit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcryptocurrency\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\\\n\u001b[1;32m      3\u001b[0m \u001b[38;5;241m.\u001b[39mwithColumnRenamed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated_utc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      4\u001b[0m \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated_utc\u001b[39m\u001b[38;5;124m\"\u001b[39m, f\u001b[38;5;241m.\u001b[39mdate_format(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated_utc\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myyyy-MM-dd HH:00:00\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      6\u001b[0m columns \u001b[38;5;241m=\u001b[39m [f\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxs\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreated_utc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcryptocurrency\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword_count\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupvotes\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomments\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m prediction \u001b[38;5;241m=\u001b[39m (prediction\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxs\u001b[39m\u001b[38;5;124m\"\u001b[39m, vector_to_array(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprobability\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\u001b[38;5;241m.\u001b[39mselect(columns)\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py:2475\u001b[0m, in \u001b[0;36mDataFrame.withColumnRenamed\u001b[0;34m(self, existing, new)\u001b[0m\n\u001b[1;32m   2457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwithColumnRenamed\u001b[39m(\u001b[38;5;28mself\u001b[39m, existing, new):\n\u001b[1;32m   2458\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns a new :class:`DataFrame` by renaming an existing column.\u001b[39;00m\n\u001b[1;32m   2459\u001b[0m \u001b[38;5;124;03m    This is a no-op if schema doesn't contain the given column name.\u001b[39;00m\n\u001b[1;32m   2460\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2473\u001b[0m \u001b[38;5;124;03m    [Row(age2=2, name='Alice'), Row(age2=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m   2474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2475\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumnRenamed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexisting\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msql_ctx)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1303\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1296\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1305\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1031\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1031\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1033\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:979\u001b[0m, in \u001b[0;36mGatewayClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    977\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeque\u001b[38;5;241m.\u001b[39mpop()\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[0;32m--> 979\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:985\u001b[0m, in \u001b[0;36mGatewayClient._create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    983\u001b[0m     connection \u001b[38;5;241m=\u001b[39m GatewayConnection(\n\u001b[1;32m    984\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property)\n\u001b[0;32m--> 985\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    986\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1127\u001b[0m, in \u001b[0;36mGatewayConnection.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1124\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while trying to connect to the Java \u001b[39m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[1;32m   1125\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserver (\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maddress, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport)\n\u001b[1;32m   1126\u001b[0m logger\u001b[38;5;241m.\u001b[39mexception(msg)\n\u001b[0;32m-> 1127\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Py4JNetworkError(msg, e)\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m: An error occurred while trying to connect to the Java server (127.0.0.1:46023)"
     ]
    }
   ],
   "source": [
    "prediction = prediction_raw\\\n",
    ".withColumnRenamed(\"subreddit\", \"cryptocurrency\")\\\n",
    ".withColumnRenamed(\"time\", \"created_utc\")\\\n",
    ".withColumn(\"created_utc\", f.date_format(col(\"created_utc\"), \"yyyy-MM-dd HH:00:00\"))\n",
    "\n",
    "columns = [f.col(\"xs\")[1], 'created_utc', 'cryptocurrency', 'word_count', 'upvotes', 'comments']\n",
    "prediction = (prediction.withColumn(\"xs\", vector_to_array(\"probability\"))).select(columns)\n",
    "prediction = prediction.withColumnRenamed('xs[1]', 'sentiment')\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2726eb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/22 19:09:32 WARN org.apache.spark.sql.streaming.StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-ed9e9618-b201-4bf7-ba68-ee8bfd9b5d12. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/01/22 19:09:32 WARN org.apache.spark.sql.streaming.StreamingQueryManager: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "[Stage 12:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------------+--------------+----------+-------+--------+\n",
      "|prediction|         probability|        created_utc|cryptocurrency|word_count|upvotes|comments|\n",
      "+----------+--------------------+-------------------+--------------+----------+-------+--------+\n",
      "|       1.0|  0.6202467984518677|2023-01-22 11:00:00|       bitcoin|         7|      1|       0|\n",
      "|       1.0|  0.6202467984518677|2023-01-22 12:00:00|       bitcoin|         2|      1|       0|\n",
      "|       1.0|  0.9999936424766556|2023-01-22 13:00:00|       bitcoin|        90|      1|       0|\n",
      "|       1.0|  0.6202467984518677|2023-01-22 13:00:00|      dogecoin|         3|      1|       0|\n",
      "|       1.0|  0.6202467984518677|2023-01-22 11:00:00|       bitcoin|        24|      1|       0|\n",
      "|       1.0|  0.8683240493020469|2023-01-22 13:00:00|       bitcoin|        82|      1|       0|\n",
      "|       1.0|  0.9999804915117282|2023-01-22 13:00:00|       cardano|       105|      1|       1|\n",
      "|       1.0|  0.6202467984518677|2023-01-22 13:00:00|      dogecoin|         5|      1|       0|\n",
      "|       1.0|  0.6202467984518677|2023-01-22 13:00:00|       bitcoin|         9|      1|       0|\n",
      "|       1.0|  0.9214723867592508|2023-01-22 12:00:00|       bitcoin|        44|      1|       0|\n",
      "|       0.0|1.893596390800667...|2023-01-22 13:00:00|       cardano|       190|      1|       1|\n",
      "|       1.0|  0.6202467984518677|2023-01-22 11:00:00|        solana|         7|      1|       1|\n",
      "|       1.0|                 1.0|2023-01-22 11:00:00|       bitcoin|       193|      1|       0|\n",
      "|       1.0|  0.9737915594456064|2023-01-22 11:00:00|       bitcoin|        27|      1|       0|\n",
      "|       1.0|  0.6202467984518677|2023-01-22 11:00:00|      ethereum|        11|      1|       0|\n",
      "|       1.0|  0.6202467984518677|2023-01-22 11:00:00|       cardano|         9|      1|       1|\n",
      "|       1.0|  0.6202467984518677|2023-01-22 12:00:00|       bitcoin|        10|      1|       0|\n",
      "|       1.0|  0.6202467984518677|2023-01-22 12:00:00|       bitcoin|        17|      1|       0|\n",
      "|       0.0|  0.4332168906433792|2023-01-22 12:00:00|       bitcoin|        80|      1|       0|\n",
      "|       1.0|  0.6202467984518677|2023-01-22 13:00:00|      dogecoin|         9|      1|       0|\n",
      "+----------+--------------------+-------------------+--------------+----------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/22 19:09:37 WARN org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 3577 milliseconds\n"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "    prediction.writeStream.format(\"console\")\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(processingTime=\"1 second\")\n",
    "    .start()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
