{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d228de1d-498a-493e-8160-16cbc9452cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_number = 1072423212419\n",
    "project_id = \"crypto-busting-375023\"\n",
    "location = \"europe-central2\"\n",
    "subscription_id = \"bda-coinbase-topic-sub\"\n",
    "topic_id = \"bda-coinbase-topic\"\n",
    "timeout = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0387738c-8c63-4fb5-a533-20a2f1e7f150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import TimeoutError\n",
    "from google.cloud import pubsub_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dacf3082-1cae-4890-aea1-a78ecb1b3c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "subscriber = pubsub_v1.SubscriberClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "347c6f81-80b6-4b5d-a0b2-3524173ac535",
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_path = subscriber.subscription_path(project_id, subscription_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18a3fe13-56a9-4b48-8fdf-40211cf8b975",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afeeb2f6-6b6f-44e9-a7a3-e696f6aa8bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(message: pubsub_v1.subscriber.message.Message) -> None:\n",
    "    print(f\"Received {message}.\")\n",
    "    message.ack()\n",
    "    messages.append(message.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0125956-6609-4de7-b7b4-4c5d28ad3527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening for messages on projects/crypto-busting-375023/subscriptions/bda-coinbase-topic-sub..\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streaming_pull_future = subscriber.subscribe(subscription_path, callback=callback)\n",
    "print(f\"Listening for messages on {subscription_path}..\\n\")\n",
    "\n",
    "with subscriber:\n",
    "    try:\n",
    "        # When `timeout` is not set, result() will block indefinitely,\n",
    "        # unless an exception is encountered first.\n",
    "        streaming_pull_future.result(timeout=timeout)\n",
    "    except TimeoutError:\n",
    "        streaming_pull_future.cancel()  # Trigger the shutdown.\n",
    "        streaming_pull_future.result()  # Block until the shutdown is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66a4d95e-ee26-40cc-8ada-c4131ea4c74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d25534c3-7fa3-4f12-91e6-8ff25c97fd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mes in messages:\n",
    "    mes.decode('UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1bd7523-651c-42f6-bc05-03028624eb9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'side': '',\n",
       " 'price': '1549.4',\n",
       " 'product_id': 'ETH-USD',\n",
       " 'time': '2023-01-20T12:04:14.537578Z'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = [ast.literal_eval(mes.decode(\"UTF-8\")) for mes in messages]\n",
    "response[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c60a4c-cacc-4ef1-b787-4a34361428f1",
   "metadata": {},
   "source": [
    "# Pub/Sub Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "036bbabd-4bdc-4387-891a-eb62ce93c202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-pubsub==2.10 in /opt/conda/miniconda3/lib/python3.8/site-packages (2.10.0)\n",
      "Requirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.28.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-cloud-pubsub==2.10) (1.34.0)\n",
      "Requirement already satisfied: grpcio-status>=1.16.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-cloud-pubsub==2.10) (1.48.2)\n",
      "Requirement already satisfied: proto-plus>=1.7.1 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-cloud-pubsub==2.10) (1.22.2)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.38.1 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-cloud-pubsub==2.10) (1.51.1)\n",
      "Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-cloud-pubsub==2.10) (0.12.4)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.28.0->google-cloud-pubsub==2.10) (2.25.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.28.0->google-cloud-pubsub==2.10) (1.57.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.28.0->google-cloud-pubsub==2.10) (1.35.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.28.0->google-cloud-pubsub==2.10) (3.20.3)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.28.0->google-cloud-pubsub==2.10) (59.8.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.28.0->google-cloud-pubsub==2.10) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.28.0->google-cloud-pubsub==2.10) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.28.0->google-cloud-pubsub==2.10) (0.2.7)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.28.0->google-cloud-pubsub==2.10) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.28.0->google-cloud-pubsub==2.10) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.28.0->google-cloud-pubsub==2.10) (2022.12.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.28.0->google-cloud-pubsub==2.10) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.28.0->google-cloud-pubsub==2.10) (4.0.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/miniconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.28.0->google-cloud-pubsub==2.10) (0.4.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install google-cloud-pubsub==2.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03bfbbe8-0826-4566-bedf-1a80d859266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud.pubsublite.cloudpubsub import PublisherClient\n",
    "from google.cloud.pubsublite.types import (\n",
    "    CloudRegion,\n",
    "    CloudZone,\n",
    "    MessageMetadata,\n",
    "    TopicPath,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a99d2fa-2fb7-4859-8996-7f79779fc832",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_id = \"bda-coinbase-topic-lite\"\n",
    "subscription_id = \"bda-coinbase-sub-lite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75d71853-7ade-462d-b0ba-8d38791a88ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = CloudRegion(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b23f108f-b347-43ed-839d-34a2749dee0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CloudRegion(name='europe-central2')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc.region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "192a37c1-c036-4671-a722-2c30c00eb210",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_path = TopicPath(project_number, loc, topic_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "439fa647-545a-4260-b47f-3dbfbf9c4968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published a message to projects/1072423212419/locations/europe-central2/topics/bda-coinbase-topic-lite with partition 0 and offset 10.\n",
      "Published a message to projects/1072423212419/locations/europe-central2/topics/bda-coinbase-topic-lite with partition 0 and offset 11.\n",
      "Published a message to projects/1072423212419/locations/europe-central2/topics/bda-coinbase-topic-lite with partition 0 and offset 12.\n",
      "Published a message to projects/1072423212419/locations/europe-central2/topics/bda-coinbase-topic-lite with partition 0 and offset 13.\n",
      "Published a message to projects/1072423212419/locations/europe-central2/topics/bda-coinbase-topic-lite with partition 0 and offset 14.\n",
      "Published a message to projects/1072423212419/locations/europe-central2/topics/bda-coinbase-topic-lite with partition 0 and offset 15.\n",
      "Published a message to projects/1072423212419/locations/europe-central2/topics/bda-coinbase-topic-lite with partition 0 and offset 16.\n",
      "Published a message to projects/1072423212419/locations/europe-central2/topics/bda-coinbase-topic-lite with partition 0 and offset 17.\n",
      "Published a message to projects/1072423212419/locations/europe-central2/topics/bda-coinbase-topic-lite with partition 0 and offset 18.\n",
      "Published a message to projects/1072423212419/locations/europe-central2/topics/bda-coinbase-topic-lite with partition 0 and offset 19.\n",
      "Published a message to projects/1072423212419/locations/europe-central2/topics/bda-coinbase-topic-lite with partition 0 and offset 20.\n",
      "Published a message to projects/1072423212419/locations/europe-central2/topics/bda-coinbase-topic-lite with partition 0 and offset 21.\n",
      "Published a message to projects/1072423212419/locations/europe-central2/topics/bda-coinbase-topic-lite with partition 0 and offset 22.\n",
      "Published a message to projects/1072423212419/locations/europe-central2/topics/bda-coinbase-topic-lite with partition 0 and offset 23.\n",
      "Published a message to projects/1072423212419/locations/europe-central2/topics/bda-coinbase-topic-lite with partition 0 and offset 24.\n"
     ]
    }
   ],
   "source": [
    "with PublisherClient() as publisher_client:\n",
    "    for msg in messages:\n",
    "        api_future = publisher_client.publish(topic_path, msg)\n",
    "        # result() blocks. To resolve API futures asynchronously, use add_done_callback().\n",
    "        message_id = api_future.result()\n",
    "        message_metadata = MessageMetadata.decode(message_id)\n",
    "        print(\n",
    "            f\"Published a message to {topic_path} with partition {message_metadata.partition.value} and offset {message_metadata.cursor.offset}.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfe3ae2-2467-4a50-a702-b77e9f14e083",
   "metadata": {},
   "source": [
    "# Pub/Sub Lite to SparkStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b992ef3-fd87-4f1d-a19b-8a96f85213ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bfae20d-cd97-49fa-aeee-6753b6c4cfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/01/20 12:13:06 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "23/01/20 12:13:06 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "23/01/20 12:13:06 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/01/20 12:13:06 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Read Pub/Sub Stream\").master(\"yarn\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "302b5f7a-96d1-4520-87ca-b3cc345c1338",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n",
      "23/01/20 12:13:20 WARN org.apache.hadoop.hive.ql.session.SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[result: int]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.sql('add jar file:////home/bda_reddit/dependancies/spark-streaming-pubsub_2.11.jar')\n",
    "# spark.sql('add jar file:////home/bda_reddit/dependancies/spark-streaming_2.13-3.3.1.jar')\n",
    "# spark.sql('add jar file:////home/bda_reddit/repos/BigDataAnalytics/2_data_preprocessing/pubsublite-spark-sql-streaming-0.4.2.jar')\n",
    "# spark.sql('add jar file:////home/bda_reddit/dependancies/spark-catalyst_2.13-3.3.1.jar')\n",
    "spark.sql(\"add jar gs://spark-lib/pubsublite/pubsublite-spark-sql-streaming-LATEST-with-dependencies.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be5620a6-a470-4f28-838b-8fffd108e407",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"add jar file:////home/bda_reddit/dependancies/gax-grpc-1.53.1.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf3b4db7-f072-425d-b448-3db0caf17902",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = (\n",
    "    spark.readStream.format(\"pubsublite\")\n",
    "    .option(\n",
    "        \"pubsublite.subscription\",\n",
    "        f\"projects/{project_number}/locations/{location}/subscriptions/{subscription_id}\",\n",
    "    )\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "823afdb3-583e-45ed-be64-6b514adb9697",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/20 12:13:31 WARN org.apache.spark.sql.streaming.StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-d51b647d-9358-48e8-ba4b-d6418be07e95. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/01/20 12:13:31 WARN org.apache.spark.sql.streaming.StreamingQueryManager: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f1c208a6c10>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.writeStream.format(\"console\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed30d137-1090-4557-ada3-bb239923c9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/20 12:13:35 WARN org.apache.spark.sql.streaming.StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-b3a8385d-8212-4e27-b3ea-de012952e870. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/01/20 12:13:35 WARN org.apache.spark.sql.streaming.StreamingQueryManager: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/01/20 12:13:48 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (bda-hdfs-w-1.europe-central2-a.c.crypto-busting-375023.internal executor 1): org.apache.spark.util.TaskCompletionListenerException: \n",
      "\n",
      "Previous exception in task: Failed to retrieve messages.\n",
      "\tcom.google.cloud.pubsublite.spark.PslMicroBatchInputPartitionReader.next(PslMicroBatchInputPartitionReader.java:74)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:79)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:112)\n",
      "\torg.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tscala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\torg.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\torg.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\torg.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:413)\n",
      "\torg.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:452)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:360)\n",
      "\torg.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\torg.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\torg.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n",
      "\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\torg.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n",
      "\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tjava.lang.Thread.run(Thread.java:750)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:145)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "23/01/20 12:13:48 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1) (bda-hdfs-w-1.europe-central2-a.c.crypto-busting-375023.internal executor 1): org.apache.spark.util.TaskCompletionListenerException: \n",
      "\n",
      "Previous exception in task: Failed to retrieve messages.\n",
      "\tcom.google.cloud.pubsublite.spark.PslMicroBatchInputPartitionReader.next(PslMicroBatchInputPartitionReader.java:74)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:79)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:112)\n",
      "\torg.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tscala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\torg.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\torg.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\torg.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:413)\n",
      "\torg.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:452)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:360)\n",
      "\torg.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\torg.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\torg.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n",
      "\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\torg.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n",
      "\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tjava.lang.Thread.run(Thread.java:750)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:145)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "23/01/20 12:13:58 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.3 in stage 0.0 (TID 6) (bda-hdfs-w-0.europe-central2-a.c.crypto-busting-375023.internal executor 2): org.apache.spark.util.TaskCompletionListenerException: \n",
      "\n",
      "Previous exception in task: Failed to retrieve messages.\n",
      "\tcom.google.cloud.pubsublite.spark.PslMicroBatchInputPartitionReader.next(PslMicroBatchInputPartitionReader.java:74)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:79)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:112)\n",
      "\torg.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tscala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\torg.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\torg.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\torg.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:413)\n",
      "\torg.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:452)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:360)\n",
      "\torg.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\torg.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\torg.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n",
      "\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\torg.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n",
      "\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tjava.lang.Thread.run(Thread.java:750)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:145)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "23/01/20 12:13:58 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job\n",
      "23/01/20 12:13:58 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.3 in stage 1.0 (TID 7) (bda-hdfs-w-1.europe-central2-a.c.crypto-busting-375023.internal executor 1): org.apache.spark.util.TaskCompletionListenerException: \n",
      "\n",
      "Previous exception in task: Failed to retrieve messages.\n",
      "\tcom.google.cloud.pubsublite.spark.PslMicroBatchInputPartitionReader.next(PslMicroBatchInputPartitionReader.java:74)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:79)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:112)\n",
      "\torg.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tscala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\torg.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\torg.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\torg.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:413)\n",
      "\torg.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:452)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:360)\n",
      "\torg.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\torg.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\torg.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n",
      "\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\torg.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n",
      "\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tjava.lang.Thread.run(Thread.java:750)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:145)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "23/01/20 12:13:58 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 1.0 failed 4 times; aborting job\n",
      "23/01/20 12:13:58 ERROR org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@4303d999 is aborting.\n",
      "23/01/20 12:13:58 ERROR org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@4303d999 aborted.\n",
      "23/01/20 12:13:58 ERROR org.apache.spark.sql.execution.streaming.MicroBatchExecution: Query [id = 39dbbb00-0f89-4f2e-8ef1-79107ed143d6, runId = 9d2ffab2-65ab-4873-ad05-bd8e12e4d298] terminated with error\n",
      "org.apache.spark.SparkException: Writing job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:336)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:297)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:40)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:40)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:46)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3709)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2978)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3700)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3698)\n",
      "\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:2978)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:589)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$15(MicroBatchExecution.scala:584)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:357)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:355)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:584)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:226)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:357)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:355)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:194)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:334)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6) (bda-hdfs-w-0.europe-central2-a.c.crypto-busting-375023.internal executor 2): org.apache.spark.util.TaskCompletionListenerException: \n",
      "\n",
      "Previous exception in task: Failed to retrieve messages.\n",
      "\tcom.google.cloud.pubsublite.spark.PslMicroBatchInputPartitionReader.next(PslMicroBatchInputPartitionReader.java:74)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:79)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:112)\n",
      "\torg.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tscala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\torg.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\torg.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\torg.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:413)\n",
      "\torg.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:452)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:360)\n",
      "\torg.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\torg.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\torg.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n",
      "\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\torg.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n",
      "\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tjava.lang.Thread.run(Thread.java:750)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:145)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2204)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:357)\n",
      "\t... 40 more\n",
      "23/01/20 12:13:58 ERROR org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@a41a750 is aborting.\n",
      "23/01/20 12:13:58 ERROR org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@a41a750 aborted.\n",
      "23/01/20 12:13:58 ERROR org.apache.spark.sql.execution.streaming.MicroBatchExecution: Query [id = d353d59b-0eea-4674-b890-377699fcbb2b, runId = 067333a9-571f-45a2-be0e-e31980e9c551] terminated with error\n",
      "org.apache.spark.SparkException: Writing job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:336)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:297)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:40)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:40)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:46)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3709)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2978)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3700)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3698)\n",
      "\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:2978)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:589)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$15(MicroBatchExecution.scala:584)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:357)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:355)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:584)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:226)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:357)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:355)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:194)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:334)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 7) (bda-hdfs-w-1.europe-central2-a.c.crypto-busting-375023.internal executor 1): org.apache.spark.util.TaskCompletionListenerException: \n",
      "\n",
      "Previous exception in task: Failed to retrieve messages.\n",
      "\tcom.google.cloud.pubsublite.spark.PslMicroBatchInputPartitionReader.next(PslMicroBatchInputPartitionReader.java:74)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:79)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:112)\n",
      "\torg.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tscala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\torg.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\torg.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\torg.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:413)\n",
      "\torg.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:452)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:360)\n",
      "\torg.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\torg.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\torg.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n",
      "\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\torg.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n",
      "\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tjava.lang.Thread.run(Thread.java:750)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:145)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2204)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:357)\n",
      "\t... 40 more\n"
     ]
    },
    {
     "ename": "StreamingQueryException",
     "evalue": "Writing job aborted.\n=== Streaming Query ===\nIdentifier: [id = d353d59b-0eea-4674-b890-377699fcbb2b, runId = 067333a9-571f-45a2-be0e-e31980e9c551]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {com.google.cloud.pubsublite.spark.PslMicroBatchStream@1d4d79c3: {\"0\":24}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nWriteToMicroBatchDataSource ConsoleWriter[numRows=20, truncate=true]\n+- StreamingDataSourceV2Relation [subscription#5, partition#6L, offset#7L, key#8, data#9, publish_timestamp#10, event_timestamp#11, attributes#12], com.google.cloud.pubsublite.spark.PslScanBuilder@ed1422, com.google.cloud.pubsublite.spark.PslMicroBatchStream@1d4d79c3\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[43msdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconsole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputMode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/streaming.py:101\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: Writing job aborted.\n=== Streaming Query ===\nIdentifier: [id = d353d59b-0eea-4674-b890-377699fcbb2b, runId = 067333a9-571f-45a2-be0e-e31980e9c551]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {com.google.cloud.pubsublite.spark.PslMicroBatchStream@1d4d79c3: {\"0\":24}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nWriteToMicroBatchDataSource ConsoleWriter[numRows=20, truncate=true]\n+- StreamingDataSourceV2Relation [subscription#5, partition#6L, offset#7L, key#8, data#9, publish_timestamp#10, event_timestamp#11, attributes#12], com.google.cloud.pubsublite.spark.PslScanBuilder@ed1422, com.google.cloud.pubsublite.spark.PslMicroBatchStream@1d4d79c3\n"
     ]
    }
   ],
   "source": [
    "query = sdf.writeStream.format(\"console\").outputMode(\"append\").start().awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8aadce1-d5e9-4440-a7e4-a904d5754902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/20 12:14:56 WARN org.apache.spark.sql.streaming.StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-cd23a0ba-91fa-4141-bd45-9c7aeb52b09b. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/01/20 12:14:56 WARN org.apache.spark.sql.streaming.StreamingQueryManager: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/01/20 12:14:57 ERROR com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper: *~*~*~ Channel ManagedChannelImpl{logId=529, target=europe-central2-pubsublite.googleapis.com:443} was not shutdown properly!!! ~*~*~*\n",
      "    Make sure to call shutdown()/shutdownNow() and wait until awaitTermination() returns true.\n",
      "java.lang.RuntimeException: ManagedChannel allocation site\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference.<init>(ManagedChannelOrphanWrapper.java:93)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:53)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:44)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelImplBuilder.build(ManagedChannelImplBuilder.java:630)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:297)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:388)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.<init>(ChannelPool.java:105)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:83)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:236)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:230)\n",
      "\tat com.google.api.gax.rpc.ClientContext.create(ClientContext.java:201)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.GrpcCursorServiceStub.create(GrpcCursorServiceStub.java:96)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.CursorServiceStubSettings.createStub(CursorServiceStubSettings.java:199)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.<init>(CursorServiceClient.java:153)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.create(CursorServiceClient.java:134)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorServiceClient(PslReadDataSourceOptions.java:165)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorClient(PslReadDataSourceOptions.java:179)\n",
      "\tat com.google.cloud.pubsublite.spark.PslMicroBatchStream.<init>(PslMicroBatchStream.java:55)\n",
      "\tat com.google.cloud.pubsublite.spark.PslScanBuilder.toMicroBatchStream(PslScanBuilder.java:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:173)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\n",
      "23/01/20 12:14:57 ERROR com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper: *~*~*~ Channel ManagedChannelImpl{logId=533, target=europe-central2-pubsublite.googleapis.com:443} was not shutdown properly!!! ~*~*~*\n",
      "    Make sure to call shutdown()/shutdownNow() and wait until awaitTermination() returns true.\n",
      "java.lang.RuntimeException: ManagedChannel allocation site\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference.<init>(ManagedChannelOrphanWrapper.java:93)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:53)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:44)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelImplBuilder.build(ManagedChannelImplBuilder.java:630)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:297)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:388)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.<init>(ChannelPool.java:105)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:83)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:236)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:230)\n",
      "\tat com.google.api.gax.rpc.ClientContext.create(ClientContext.java:201)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.GrpcCursorServiceStub.create(GrpcCursorServiceStub.java:96)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.CursorServiceStubSettings.createStub(CursorServiceStubSettings.java:199)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.<init>(CursorServiceClient.java:153)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.create(CursorServiceClient.java:134)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorServiceClient(PslReadDataSourceOptions.java:165)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorClient(PslReadDataSourceOptions.java:179)\n",
      "\tat com.google.cloud.pubsublite.spark.PslMicroBatchStream.<init>(PslMicroBatchStream.java:55)\n",
      "\tat com.google.cloud.pubsublite.spark.PslScanBuilder.toMicroBatchStream(PslScanBuilder.java:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:173)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\n",
      "23/01/20 12:14:57 ERROR com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper: *~*~*~ Channel ManagedChannelImpl{logId=527, target=europe-central2-pubsublite.googleapis.com:443} was not shutdown properly!!! ~*~*~*\n",
      "    Make sure to call shutdown()/shutdownNow() and wait until awaitTermination() returns true.\n",
      "java.lang.RuntimeException: ManagedChannel allocation site\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference.<init>(ManagedChannelOrphanWrapper.java:93)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:53)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:44)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelImplBuilder.build(ManagedChannelImplBuilder.java:630)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:297)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:388)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.<init>(ChannelPool.java:105)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:83)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:236)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:230)\n",
      "\tat com.google.api.gax.rpc.ClientContext.create(ClientContext.java:201)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.GrpcCursorServiceStub.create(GrpcCursorServiceStub.java:96)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.CursorServiceStubSettings.createStub(CursorServiceStubSettings.java:199)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.<init>(CursorServiceClient.java:153)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.create(CursorServiceClient.java:134)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorServiceClient(PslReadDataSourceOptions.java:165)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorClient(PslReadDataSourceOptions.java:179)\n",
      "\tat com.google.cloud.pubsublite.spark.PslMicroBatchStream.<init>(PslMicroBatchStream.java:55)\n",
      "\tat com.google.cloud.pubsublite.spark.PslScanBuilder.toMicroBatchStream(PslScanBuilder.java:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:173)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\n",
      "23/01/20 12:14:57 ERROR com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper: *~*~*~ Channel ManagedChannelImpl{logId=537, target=europe-central2-pubsublite.googleapis.com:443} was not shutdown properly!!! ~*~*~*\n",
      "    Make sure to call shutdown()/shutdownNow() and wait until awaitTermination() returns true.\n",
      "java.lang.RuntimeException: ManagedChannel allocation site\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference.<init>(ManagedChannelOrphanWrapper.java:93)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:53)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:44)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelImplBuilder.build(ManagedChannelImplBuilder.java:630)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:297)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:388)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.<init>(ChannelPool.java:105)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:83)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:236)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:230)\n",
      "\tat com.google.api.gax.rpc.ClientContext.create(ClientContext.java:201)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.GrpcCursorServiceStub.create(GrpcCursorServiceStub.java:96)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.CursorServiceStubSettings.createStub(CursorServiceStubSettings.java:199)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.<init>(CursorServiceClient.java:153)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.create(CursorServiceClient.java:134)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorServiceClient(PslReadDataSourceOptions.java:165)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorClient(PslReadDataSourceOptions.java:179)\n",
      "\tat com.google.cloud.pubsublite.spark.PslMicroBatchStream.<init>(PslMicroBatchStream.java:55)\n",
      "\tat com.google.cloud.pubsublite.spark.PslScanBuilder.toMicroBatchStream(PslScanBuilder.java:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:173)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\n",
      "23/01/20 12:14:57 ERROR com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper: *~*~*~ Channel ManagedChannelImpl{logId=581, target=europe-central2-pubsublite.googleapis.com:443} was not shutdown properly!!! ~*~*~*\n",
      "    Make sure to call shutdown()/shutdownNow() and wait until awaitTermination() returns true.\n",
      "java.lang.RuntimeException: ManagedChannel allocation site\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference.<init>(ManagedChannelOrphanWrapper.java:93)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:53)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:44)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelImplBuilder.build(ManagedChannelImplBuilder.java:630)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:297)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:388)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.<init>(ChannelPool.java:105)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:83)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:236)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:230)\n",
      "\tat com.google.api.gax.rpc.ClientContext.create(ClientContext.java:201)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.GrpcTopicStatsServiceStub.create(GrpcTopicStatsServiceStub.java:93)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.TopicStatsServiceStubSettings.createStub(TopicStatsServiceStubSettings.java:120)\n",
      "\tat com.google.cloud.pubsublite.v1.TopicStatsServiceClient.<init>(TopicStatsServiceClient.java:144)\n",
      "\tat com.google.cloud.pubsublite.v1.TopicStatsServiceClient.create(TopicStatsServiceClient.java:125)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newTopicStatsServiceClient(PslReadDataSourceOptions.java:205)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newTopicStatsClient(PslReadDataSourceOptions.java:227)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newHeadOffsetReader(PslReadDataSourceOptions.java:237)\n",
      "\tat com.google.cloud.pubsublite.spark.PslMicroBatchStream.<init>(PslMicroBatchStream.java:57)\n",
      "\tat com.google.cloud.pubsublite.spark.PslScanBuilder.toMicroBatchStream(PslScanBuilder.java:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:173)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\n",
      "23/01/20 12:14:57 ERROR com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper: *~*~*~ Channel ManagedChannelImpl{logId=531, target=europe-central2-pubsublite.googleapis.com:443} was not shutdown properly!!! ~*~*~*\n",
      "    Make sure to call shutdown()/shutdownNow() and wait until awaitTermination() returns true.\n",
      "java.lang.RuntimeException: ManagedChannel allocation site\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference.<init>(ManagedChannelOrphanWrapper.java:93)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:53)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:44)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelImplBuilder.build(ManagedChannelImplBuilder.java:630)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:297)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:388)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.<init>(ChannelPool.java:105)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:83)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:236)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:230)\n",
      "\tat com.google.api.gax.rpc.ClientContext.create(ClientContext.java:201)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.GrpcCursorServiceStub.create(GrpcCursorServiceStub.java:96)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.CursorServiceStubSettings.createStub(CursorServiceStubSettings.java:199)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.<init>(CursorServiceClient.java:153)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.create(CursorServiceClient.java:134)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorServiceClient(PslReadDataSourceOptions.java:165)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorClient(PslReadDataSourceOptions.java:179)\n",
      "\tat com.google.cloud.pubsublite.spark.PslMicroBatchStream.<init>(PslMicroBatchStream.java:55)\n",
      "\tat com.google.cloud.pubsublite.spark.PslScanBuilder.toMicroBatchStream(PslScanBuilder.java:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:173)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\n",
      "23/01/20 12:14:57 ERROR com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper: *~*~*~ Channel ManagedChannelImpl{logId=577, target=europe-central2-pubsublite.googleapis.com:443} was not shutdown properly!!! ~*~*~*\n",
      "    Make sure to call shutdown()/shutdownNow() and wait until awaitTermination() returns true.\n",
      "java.lang.RuntimeException: ManagedChannel allocation site\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference.<init>(ManagedChannelOrphanWrapper.java:93)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:53)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:44)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelImplBuilder.build(ManagedChannelImplBuilder.java:630)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:297)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:388)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.<init>(ChannelPool.java:105)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:83)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:236)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:230)\n",
      "\tat com.google.api.gax.rpc.ClientContext.create(ClientContext.java:201)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.GrpcTopicStatsServiceStub.create(GrpcTopicStatsServiceStub.java:93)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.TopicStatsServiceStubSettings.createStub(TopicStatsServiceStubSettings.java:120)\n",
      "\tat com.google.cloud.pubsublite.v1.TopicStatsServiceClient.<init>(TopicStatsServiceClient.java:144)\n",
      "\tat com.google.cloud.pubsublite.v1.TopicStatsServiceClient.create(TopicStatsServiceClient.java:125)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newTopicStatsServiceClient(PslReadDataSourceOptions.java:205)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newTopicStatsClient(PslReadDataSourceOptions.java:227)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newHeadOffsetReader(PslReadDataSourceOptions.java:237)\n",
      "\tat com.google.cloud.pubsublite.spark.PslMicroBatchStream.<init>(PslMicroBatchStream.java:57)\n",
      "\tat com.google.cloud.pubsublite.spark.PslScanBuilder.toMicroBatchStream(PslScanBuilder.java:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:173)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\n",
      "23/01/20 12:14:57 ERROR com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper: *~*~*~ Channel ManagedChannelImpl{logId=543, target=europe-central2-pubsublite.googleapis.com:443} was not shutdown properly!!! ~*~*~*\n",
      "    Make sure to call shutdown()/shutdownNow() and wait until awaitTermination() returns true.\n",
      "java.lang.RuntimeException: ManagedChannel allocation site\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference.<init>(ManagedChannelOrphanWrapper.java:93)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:53)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:44)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelImplBuilder.build(ManagedChannelImplBuilder.java:630)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:297)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:388)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.<init>(ChannelPool.java:105)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:83)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:236)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:230)\n",
      "\tat com.google.api.gax.rpc.ClientContext.create(ClientContext.java:201)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.GrpcCursorServiceStub.create(GrpcCursorServiceStub.java:96)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.CursorServiceStubSettings.createStub(CursorServiceStubSettings.java:199)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.<init>(CursorServiceClient.java:153)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.create(CursorServiceClient.java:134)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorServiceClient(PslReadDataSourceOptions.java:165)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorClient(PslReadDataSourceOptions.java:179)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newMultiPartitionCommitter(PslReadDataSourceOptions.java:125)\n",
      "\tat com.google.cloud.pubsublite.spark.PslMicroBatchStream.<init>(PslMicroBatchStream.java:56)\n",
      "\tat com.google.cloud.pubsublite.spark.PslScanBuilder.toMicroBatchStream(PslScanBuilder.java:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:173)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\n",
      "23/01/20 12:14:57 ERROR com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper: *~*~*~ Channel ManagedChannelImpl{logId=535, target=europe-central2-pubsublite.googleapis.com:443} was not shutdown properly!!! ~*~*~*\n",
      "    Make sure to call shutdown()/shutdownNow() and wait until awaitTermination() returns true.\n",
      "java.lang.RuntimeException: ManagedChannel allocation site\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference.<init>(ManagedChannelOrphanWrapper.java:93)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:53)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:44)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelImplBuilder.build(ManagedChannelImplBuilder.java:630)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:297)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:388)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.<init>(ChannelPool.java:105)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:83)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:236)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:230)\n",
      "\tat com.google.api.gax.rpc.ClientContext.create(ClientContext.java:201)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.GrpcCursorServiceStub.create(GrpcCursorServiceStub.java:96)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.CursorServiceStubSettings.createStub(CursorServiceStubSettings.java:199)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.<init>(CursorServiceClient.java:153)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.create(CursorServiceClient.java:134)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorServiceClient(PslReadDataSourceOptions.java:165)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorClient(PslReadDataSourceOptions.java:179)\n",
      "\tat com.google.cloud.pubsublite.spark.PslMicroBatchStream.<init>(PslMicroBatchStream.java:55)\n",
      "\tat com.google.cloud.pubsublite.spark.PslScanBuilder.toMicroBatchStream(PslScanBuilder.java:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:173)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\n",
      "23/01/20 12:14:57 ERROR com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper: *~*~*~ Channel ManagedChannelImpl{logId=539, target=europe-central2-pubsublite.googleapis.com:443} was not shutdown properly!!! ~*~*~*\n",
      "    Make sure to call shutdown()/shutdownNow() and wait until awaitTermination() returns true.\n",
      "java.lang.RuntimeException: ManagedChannel allocation site\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference.<init>(ManagedChannelOrphanWrapper.java:93)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:53)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:44)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelImplBuilder.build(ManagedChannelImplBuilder.java:630)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:297)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:388)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.<init>(ChannelPool.java:105)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:83)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:236)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:230)\n",
      "\tat com.google.api.gax.rpc.ClientContext.create(ClientContext.java:201)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.GrpcCursorServiceStub.create(GrpcCursorServiceStub.java:96)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.CursorServiceStubSettings.createStub(CursorServiceStubSettings.java:199)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.<init>(CursorServiceClient.java:153)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.create(CursorServiceClient.java:134)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorServiceClient(PslReadDataSourceOptions.java:165)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorClient(PslReadDataSourceOptions.java:179)\n",
      "\tat com.google.cloud.pubsublite.spark.PslMicroBatchStream.<init>(PslMicroBatchStream.java:55)\n",
      "\tat com.google.cloud.pubsublite.spark.PslScanBuilder.toMicroBatchStream(PslScanBuilder.java:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:173)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\n",
      "23/01/20 12:14:57 ERROR com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper: *~*~*~ Channel ManagedChannelImpl{logId=575, target=europe-central2-pubsublite.googleapis.com:443} was not shutdown properly!!! ~*~*~*\n",
      "    Make sure to call shutdown()/shutdownNow() and wait until awaitTermination() returns true.\n",
      "java.lang.RuntimeException: ManagedChannel allocation site\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference.<init>(ManagedChannelOrphanWrapper.java:93)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:53)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:44)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelImplBuilder.build(ManagedChannelImplBuilder.java:630)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:297)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:388)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.<init>(ChannelPool.java:105)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:83)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:236)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:230)\n",
      "\tat com.google.api.gax.rpc.ClientContext.create(ClientContext.java:201)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.GrpcTopicStatsServiceStub.create(GrpcTopicStatsServiceStub.java:93)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.TopicStatsServiceStubSettings.createStub(TopicStatsServiceStubSettings.java:120)\n",
      "\tat com.google.cloud.pubsublite.v1.TopicStatsServiceClient.<init>(TopicStatsServiceClient.java:144)\n",
      "\tat com.google.cloud.pubsublite.v1.TopicStatsServiceClient.create(TopicStatsServiceClient.java:125)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newTopicStatsServiceClient(PslReadDataSourceOptions.java:205)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newTopicStatsClient(PslReadDataSourceOptions.java:227)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newHeadOffsetReader(PslReadDataSourceOptions.java:237)\n",
      "\tat com.google.cloud.pubsublite.spark.PslMicroBatchStream.<init>(PslMicroBatchStream.java:57)\n",
      "\tat com.google.cloud.pubsublite.spark.PslScanBuilder.toMicroBatchStream(PslScanBuilder.java:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:173)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\n",
      "23/01/20 12:14:57 ERROR com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper: *~*~*~ Channel ManagedChannelImpl{logId=579, target=europe-central2-pubsublite.googleapis.com:443} was not shutdown properly!!! ~*~*~*\n",
      "    Make sure to call shutdown()/shutdownNow() and wait until awaitTermination() returns true.\n",
      "java.lang.RuntimeException: ManagedChannel allocation site\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference.<init>(ManagedChannelOrphanWrapper.java:93)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:53)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:44)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelImplBuilder.build(ManagedChannelImplBuilder.java:630)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:297)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:388)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.<init>(ChannelPool.java:105)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:83)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:236)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:230)\n",
      "\tat com.google.api.gax.rpc.ClientContext.create(ClientContext.java:201)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.GrpcTopicStatsServiceStub.create(GrpcTopicStatsServiceStub.java:93)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.TopicStatsServiceStubSettings.createStub(TopicStatsServiceStubSettings.java:120)\n",
      "\tat com.google.cloud.pubsublite.v1.TopicStatsServiceClient.<init>(TopicStatsServiceClient.java:144)\n",
      "\tat com.google.cloud.pubsublite.v1.TopicStatsServiceClient.create(TopicStatsServiceClient.java:125)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newTopicStatsServiceClient(PslReadDataSourceOptions.java:205)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newTopicStatsClient(PslReadDataSourceOptions.java:227)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newHeadOffsetReader(PslReadDataSourceOptions.java:237)\n",
      "\tat com.google.cloud.pubsublite.spark.PslMicroBatchStream.<init>(PslMicroBatchStream.java:57)\n",
      "\tat com.google.cloud.pubsublite.spark.PslScanBuilder.toMicroBatchStream(PslScanBuilder.java:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:173)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\n",
      "23/01/20 12:14:57 ERROR com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper: *~*~*~ Channel ManagedChannelImpl{logId=573, target=europe-central2-pubsublite.googleapis.com:443} was not shutdown properly!!! ~*~*~*\n",
      "    Make sure to call shutdown()/shutdownNow() and wait until awaitTermination() returns true.\n",
      "java.lang.RuntimeException: ManagedChannel allocation site\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference.<init>(ManagedChannelOrphanWrapper.java:93)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:53)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:44)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelImplBuilder.build(ManagedChannelImplBuilder.java:630)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:297)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:388)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.<init>(ChannelPool.java:105)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:83)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:236)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:230)\n",
      "\tat com.google.api.gax.rpc.ClientContext.create(ClientContext.java:201)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.GrpcTopicStatsServiceStub.create(GrpcTopicStatsServiceStub.java:93)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.TopicStatsServiceStubSettings.createStub(TopicStatsServiceStubSettings.java:120)\n",
      "\tat com.google.cloud.pubsublite.v1.TopicStatsServiceClient.<init>(TopicStatsServiceClient.java:144)\n",
      "\tat com.google.cloud.pubsublite.v1.TopicStatsServiceClient.create(TopicStatsServiceClient.java:125)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newTopicStatsServiceClient(PslReadDataSourceOptions.java:205)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newTopicStatsClient(PslReadDataSourceOptions.java:227)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newHeadOffsetReader(PslReadDataSourceOptions.java:237)\n",
      "\tat com.google.cloud.pubsublite.spark.PslMicroBatchStream.<init>(PslMicroBatchStream.java:57)\n",
      "\tat com.google.cloud.pubsublite.spark.PslScanBuilder.toMicroBatchStream(PslScanBuilder.java:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:173)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\n",
      "23/01/20 12:14:57 ERROR com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper: *~*~*~ Channel ManagedChannelImpl{logId=553, target=europe-central2-pubsublite.googleapis.com:443} was not shutdown properly!!! ~*~*~*\n",
      "    Make sure to call shutdown()/shutdownNow() and wait until awaitTermination() returns true.\n",
      "java.lang.RuntimeException: ManagedChannel allocation site\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference.<init>(ManagedChannelOrphanWrapper.java:93)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:53)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:44)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelImplBuilder.build(ManagedChannelImplBuilder.java:630)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:297)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:388)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.<init>(ChannelPool.java:105)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:83)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:236)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:230)\n",
      "\tat com.google.api.gax.rpc.ClientContext.create(ClientContext.java:201)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.GrpcCursorServiceStub.create(GrpcCursorServiceStub.java:96)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.CursorServiceStubSettings.createStub(CursorServiceStubSettings.java:199)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.<init>(CursorServiceClient.java:153)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.create(CursorServiceClient.java:134)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorServiceClient(PslReadDataSourceOptions.java:165)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorClient(PslReadDataSourceOptions.java:179)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newMultiPartitionCommitter(PslReadDataSourceOptions.java:125)\n",
      "\tat com.google.cloud.pubsublite.spark.PslMicroBatchStream.<init>(PslMicroBatchStream.java:56)\n",
      "\tat com.google.cloud.pubsublite.spark.PslScanBuilder.toMicroBatchStream(PslScanBuilder.java:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:173)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\n",
      "23/01/20 12:14:57 ERROR com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper: *~*~*~ Channel ManagedChannelImpl{logId=549, target=europe-central2-pubsublite.googleapis.com:443} was not shutdown properly!!! ~*~*~*\n",
      "    Make sure to call shutdown()/shutdownNow() and wait until awaitTermination() returns true.\n",
      "java.lang.RuntimeException: ManagedChannel allocation site\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference.<init>(ManagedChannelOrphanWrapper.java:93)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:53)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:44)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelImplBuilder.build(ManagedChannelImplBuilder.java:630)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:297)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:388)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.<init>(ChannelPool.java:105)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:83)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:236)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:230)\n",
      "\tat com.google.api.gax.rpc.ClientContext.create(ClientContext.java:201)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.GrpcCursorServiceStub.create(GrpcCursorServiceStub.java:96)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.CursorServiceStubSettings.createStub(CursorServiceStubSettings.java:199)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.<init>(CursorServiceClient.java:153)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.create(CursorServiceClient.java:134)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorServiceClient(PslReadDataSourceOptions.java:165)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorClient(PslReadDataSourceOptions.java:179)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newMultiPartitionCommitter(PslReadDataSourceOptions.java:125)\n",
      "\tat com.google.cloud.pubsublite.spark.PslMicroBatchStream.<init>(PslMicroBatchStream.java:56)\n",
      "\tat com.google.cloud.pubsublite.spark.PslScanBuilder.toMicroBatchStream(PslScanBuilder.java:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:173)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\n",
      "23/01/20 12:14:57 ERROR com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper: *~*~*~ Channel ManagedChannelImpl{logId=557, target=europe-central2-pubsublite.googleapis.com:443} was not shutdown properly!!! ~*~*~*\n",
      "    Make sure to call shutdown()/shutdownNow() and wait until awaitTermination() returns true.\n",
      "java.lang.RuntimeException: ManagedChannel allocation site\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference.<init>(ManagedChannelOrphanWrapper.java:93)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:53)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:44)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelImplBuilder.build(ManagedChannelImplBuilder.java:630)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:297)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:388)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.<init>(ChannelPool.java:105)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:83)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:236)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:230)\n",
      "\tat com.google.api.gax.rpc.ClientContext.create(ClientContext.java:201)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.GrpcCursorServiceStub.create(GrpcCursorServiceStub.java:96)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.CursorServiceStubSettings.createStub(CursorServiceStubSettings.java:199)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.<init>(CursorServiceClient.java:153)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.create(CursorServiceClient.java:134)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorServiceClient(PslReadDataSourceOptions.java:165)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorClient(PslReadDataSourceOptions.java:179)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newMultiPartitionCommitter(PslReadDataSourceOptions.java:125)\n",
      "\tat com.google.cloud.pubsublite.spark.PslMicroBatchStream.<init>(PslMicroBatchStream.java:56)\n",
      "\tat com.google.cloud.pubsublite.spark.PslScanBuilder.toMicroBatchStream(PslScanBuilder.java:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:173)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\n",
      "23/01/20 12:14:57 ERROR com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper: *~*~*~ Channel ManagedChannelImpl{logId=571, target=europe-central2-pubsublite.googleapis.com:443} was not shutdown properly!!! ~*~*~*\n",
      "    Make sure to call shutdown()/shutdownNow() and wait until awaitTermination() returns true.\n",
      "java.lang.RuntimeException: ManagedChannel allocation site\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference.<init>(ManagedChannelOrphanWrapper.java:93)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:53)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:44)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelImplBuilder.build(ManagedChannelImplBuilder.java:630)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:297)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:388)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.<init>(ChannelPool.java:105)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:83)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:236)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:230)\n",
      "\tat com.google.api.gax.rpc.ClientContext.create(ClientContext.java:201)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.GrpcTopicStatsServiceStub.create(GrpcTopicStatsServiceStub.java:93)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.TopicStatsServiceStubSettings.createStub(TopicStatsServiceStubSettings.java:120)\n",
      "\tat com.google.cloud.pubsublite.v1.TopicStatsServiceClient.<init>(TopicStatsServiceClient.java:144)\n",
      "\tat com.google.cloud.pubsublite.v1.TopicStatsServiceClient.create(TopicStatsServiceClient.java:125)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newTopicStatsServiceClient(PslReadDataSourceOptions.java:205)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newTopicStatsClient(PslReadDataSourceOptions.java:227)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newHeadOffsetReader(PslReadDataSourceOptions.java:237)\n",
      "\tat com.google.cloud.pubsublite.spark.PslMicroBatchStream.<init>(PslMicroBatchStream.java:57)\n",
      "\tat com.google.cloud.pubsublite.spark.PslScanBuilder.toMicroBatchStream(PslScanBuilder.java:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:173)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\n",
      "23/01/20 12:14:57 ERROR com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper: *~*~*~ Channel ManagedChannelImpl{logId=563, target=europe-central2-pubsublite.googleapis.com:443} was not shutdown properly!!! ~*~*~*\n",
      "    Make sure to call shutdown()/shutdownNow() and wait until awaitTermination() returns true.\n",
      "java.lang.RuntimeException: ManagedChannel allocation site\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference.<init>(ManagedChannelOrphanWrapper.java:93)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:53)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:44)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelImplBuilder.build(ManagedChannelImplBuilder.java:630)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:297)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:388)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.<init>(ChannelPool.java:105)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:83)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:236)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:230)\n",
      "\tat com.google.api.gax.rpc.ClientContext.create(ClientContext.java:201)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.GrpcTopicStatsServiceStub.create(GrpcTopicStatsServiceStub.java:93)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.TopicStatsServiceStubSettings.createStub(TopicStatsServiceStubSettings.java:120)\n",
      "\tat com.google.cloud.pubsublite.v1.TopicStatsServiceClient.<init>(TopicStatsServiceClient.java:144)\n",
      "\tat com.google.cloud.pubsublite.v1.TopicStatsServiceClient.create(TopicStatsServiceClient.java:125)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newTopicStatsServiceClient(PslReadDataSourceOptions.java:205)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newTopicStatsClient(PslReadDataSourceOptions.java:227)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newHeadOffsetReader(PslReadDataSourceOptions.java:237)\n",
      "\tat com.google.cloud.pubsublite.spark.PslMicroBatchStream.<init>(PslMicroBatchStream.java:57)\n",
      "\tat com.google.cloud.pubsublite.spark.PslScanBuilder.toMicroBatchStream(PslScanBuilder.java:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:173)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\n",
      "23/01/20 12:14:57 ERROR com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper: *~*~*~ Channel ManagedChannelImpl{logId=567, target=europe-central2-pubsublite.googleapis.com:443} was not shutdown properly!!! ~*~*~*\n",
      "    Make sure to call shutdown()/shutdownNow() and wait until awaitTermination() returns true.\n",
      "java.lang.RuntimeException: ManagedChannel allocation site\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference.<init>(ManagedChannelOrphanWrapper.java:93)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:53)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:44)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelImplBuilder.build(ManagedChannelImplBuilder.java:630)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:297)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:388)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.<init>(ChannelPool.java:105)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:83)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:236)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:230)\n",
      "\tat com.google.api.gax.rpc.ClientContext.create(ClientContext.java:201)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.GrpcTopicStatsServiceStub.create(GrpcTopicStatsServiceStub.java:93)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.TopicStatsServiceStubSettings.createStub(TopicStatsServiceStubSettings.java:120)\n",
      "\tat com.google.cloud.pubsublite.v1.TopicStatsServiceClient.<init>(TopicStatsServiceClient.java:144)\n",
      "\tat com.google.cloud.pubsublite.v1.TopicStatsServiceClient.create(TopicStatsServiceClient.java:125)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newTopicStatsServiceClient(PslReadDataSourceOptions.java:205)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newTopicStatsClient(PslReadDataSourceOptions.java:227)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newHeadOffsetReader(PslReadDataSourceOptions.java:237)\n",
      "\tat com.google.cloud.pubsublite.spark.PslMicroBatchStream.<init>(PslMicroBatchStream.java:57)\n",
      "\tat com.google.cloud.pubsublite.spark.PslScanBuilder.toMicroBatchStream(PslScanBuilder.java:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:173)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\n",
      "23/01/20 12:14:57 ERROR com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper: *~*~*~ Channel ManagedChannelImpl{logId=555, target=europe-central2-pubsublite.googleapis.com:443} was not shutdown properly!!! ~*~*~*\n",
      "    Make sure to call shutdown()/shutdownNow() and wait until awaitTermination() returns true.\n",
      "java.lang.RuntimeException: ManagedChannel allocation site\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference.<init>(ManagedChannelOrphanWrapper.java:93)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:53)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:44)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelImplBuilder.build(ManagedChannelImplBuilder.java:630)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:297)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:388)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.<init>(ChannelPool.java:105)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:83)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:236)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:230)\n",
      "\tat com.google.api.gax.rpc.ClientContext.create(ClientContext.java:201)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.GrpcCursorServiceStub.create(GrpcCursorServiceStub.java:96)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.CursorServiceStubSettings.createStub(CursorServiceStubSettings.java:199)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.<init>(CursorServiceClient.java:153)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.create(CursorServiceClient.java:134)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorServiceClient(PslReadDataSourceOptions.java:165)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorClient(PslReadDataSourceOptions.java:179)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newMultiPartitionCommitter(PslReadDataSourceOptions.java:125)\n",
      "\tat com.google.cloud.pubsublite.spark.PslMicroBatchStream.<init>(PslMicroBatchStream.java:56)\n",
      "\tat com.google.cloud.pubsublite.spark.PslScanBuilder.toMicroBatchStream(PslScanBuilder.java:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:173)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\n",
      "23/01/20 12:14:57 ERROR com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper: *~*~*~ Channel ManagedChannelImpl{logId=541, target=europe-central2-pubsublite.googleapis.com:443} was not shutdown properly!!! ~*~*~*\n",
      "    Make sure to call shutdown()/shutdownNow() and wait until awaitTermination() returns true.\n",
      "java.lang.RuntimeException: ManagedChannel allocation site\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference.<init>(ManagedChannelOrphanWrapper.java:93)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:53)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:44)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelImplBuilder.build(ManagedChannelImplBuilder.java:630)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:297)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:388)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.<init>(ChannelPool.java:105)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:83)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:236)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:230)\n",
      "\tat com.google.api.gax.rpc.ClientContext.create(ClientContext.java:201)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.GrpcCursorServiceStub.create(GrpcCursorServiceStub.java:96)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.CursorServiceStubSettings.createStub(CursorServiceStubSettings.java:199)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.<init>(CursorServiceClient.java:153)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.create(CursorServiceClient.java:134)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorServiceClient(PslReadDataSourceOptions.java:165)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorClient(PslReadDataSourceOptions.java:179)\n",
      "\tat com.google.cloud.pubsublite.spark.PslMicroBatchStream.<init>(PslMicroBatchStream.java:55)\n",
      "\tat com.google.cloud.pubsublite.spark.PslScanBuilder.toMicroBatchStream(PslScanBuilder.java:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:173)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\n",
      "23/01/20 12:14:57 ERROR com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper: *~*~*~ Channel ManagedChannelImpl{logId=569, target=europe-central2-pubsublite.googleapis.com:443} was not shutdown properly!!! ~*~*~*\n",
      "    Make sure to call shutdown()/shutdownNow() and wait until awaitTermination() returns true.\n",
      "java.lang.RuntimeException: ManagedChannel allocation site\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference.<init>(ManagedChannelOrphanWrapper.java:93)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:53)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:44)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelImplBuilder.build(ManagedChannelImplBuilder.java:630)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:297)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:388)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.<init>(ChannelPool.java:105)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:83)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:236)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:230)\n",
      "\tat com.google.api.gax.rpc.ClientContext.create(ClientContext.java:201)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.GrpcTopicStatsServiceStub.create(GrpcTopicStatsServiceStub.java:93)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.TopicStatsServiceStubSettings.createStub(TopicStatsServiceStubSettings.java:120)\n",
      "\tat com.google.cloud.pubsublite.v1.TopicStatsServiceClient.<init>(TopicStatsServiceClient.java:144)\n",
      "\tat com.google.cloud.pubsublite.v1.TopicStatsServiceClient.create(TopicStatsServiceClient.java:125)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newTopicStatsServiceClient(PslReadDataSourceOptions.java:205)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newTopicStatsClient(PslReadDataSourceOptions.java:227)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newHeadOffsetReader(PslReadDataSourceOptions.java:237)\n",
      "\tat com.google.cloud.pubsublite.spark.PslMicroBatchStream.<init>(PslMicroBatchStream.java:57)\n",
      "\tat com.google.cloud.pubsublite.spark.PslScanBuilder.toMicroBatchStream(PslScanBuilder.java:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:173)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\n",
      "23/01/20 12:14:57 ERROR com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper: *~*~*~ Channel ManagedChannelImpl{logId=525, target=europe-central2-pubsublite.googleapis.com:443} was not shutdown properly!!! ~*~*~*\n",
      "    Make sure to call shutdown()/shutdownNow() and wait until awaitTermination() returns true.\n",
      "java.lang.RuntimeException: ManagedChannel allocation site\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference.<init>(ManagedChannelOrphanWrapper.java:93)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:53)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:44)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelImplBuilder.build(ManagedChannelImplBuilder.java:630)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:297)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:388)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.<init>(ChannelPool.java:105)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:83)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:236)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:230)\n",
      "\tat com.google.api.gax.rpc.ClientContext.create(ClientContext.java:201)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.GrpcCursorServiceStub.create(GrpcCursorServiceStub.java:96)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.CursorServiceStubSettings.createStub(CursorServiceStubSettings.java:199)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.<init>(CursorServiceClient.java:153)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.create(CursorServiceClient.java:134)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorServiceClient(PslReadDataSourceOptions.java:165)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorClient(PslReadDataSourceOptions.java:179)\n",
      "\tat com.google.cloud.pubsublite.spark.PslMicroBatchStream.<init>(PslMicroBatchStream.java:55)\n",
      "\tat com.google.cloud.pubsublite.spark.PslScanBuilder.toMicroBatchStream(PslScanBuilder.java:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:173)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\n",
      "23/01/20 12:14:57 ERROR com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper: *~*~*~ Channel ManagedChannelImpl{logId=559, target=europe-central2-pubsublite.googleapis.com:443} was not shutdown properly!!! ~*~*~*\n",
      "    Make sure to call shutdown()/shutdownNow() and wait until awaitTermination() returns true.\n",
      "java.lang.RuntimeException: ManagedChannel allocation site\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference.<init>(ManagedChannelOrphanWrapper.java:93)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:53)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:44)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelImplBuilder.build(ManagedChannelImplBuilder.java:630)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:297)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:388)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.<init>(ChannelPool.java:105)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:83)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:236)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:230)\n",
      "\tat com.google.api.gax.rpc.ClientContext.create(ClientContext.java:201)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.GrpcCursorServiceStub.create(GrpcCursorServiceStub.java:96)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.CursorServiceStubSettings.createStub(CursorServiceStubSettings.java:199)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.<init>(CursorServiceClient.java:153)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.create(CursorServiceClient.java:134)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorServiceClient(PslReadDataSourceOptions.java:165)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorClient(PslReadDataSourceOptions.java:179)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newMultiPartitionCommitter(PslReadDataSourceOptions.java:125)\n",
      "\tat com.google.cloud.pubsublite.spark.PslMicroBatchStream.<init>(PslMicroBatchStream.java:56)\n",
      "\tat com.google.cloud.pubsublite.spark.PslScanBuilder.toMicroBatchStream(PslScanBuilder.java:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:173)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\n",
      "23/01/20 12:14:57 ERROR com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper: *~*~*~ Channel ManagedChannelImpl{logId=551, target=europe-central2-pubsublite.googleapis.com:443} was not shutdown properly!!! ~*~*~*\n",
      "    Make sure to call shutdown()/shutdownNow() and wait until awaitTermination() returns true.\n",
      "java.lang.RuntimeException: ManagedChannel allocation site\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference.<init>(ManagedChannelOrphanWrapper.java:93)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:53)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:44)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelImplBuilder.build(ManagedChannelImplBuilder.java:630)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:297)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:388)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.<init>(ChannelPool.java:105)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:83)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:236)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:230)\n",
      "\tat com.google.api.gax.rpc.ClientContext.create(ClientContext.java:201)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.GrpcCursorServiceStub.create(GrpcCursorServiceStub.java:96)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.CursorServiceStubSettings.createStub(CursorServiceStubSettings.java:199)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.<init>(CursorServiceClient.java:153)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.create(CursorServiceClient.java:134)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorServiceClient(PslReadDataSourceOptions.java:165)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorClient(PslReadDataSourceOptions.java:179)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newMultiPartitionCommitter(PslReadDataSourceOptions.java:125)\n",
      "\tat com.google.cloud.pubsublite.spark.PslMicroBatchStream.<init>(PslMicroBatchStream.java:56)\n",
      "\tat com.google.cloud.pubsublite.spark.PslScanBuilder.toMicroBatchStream(PslScanBuilder.java:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:173)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\n",
      "23/01/20 12:14:57 ERROR com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper: *~*~*~ Channel ManagedChannelImpl{logId=565, target=europe-central2-pubsublite.googleapis.com:443} was not shutdown properly!!! ~*~*~*\n",
      "    Make sure to call shutdown()/shutdownNow() and wait until awaitTermination() returns true.\n",
      "java.lang.RuntimeException: ManagedChannel allocation site\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference.<init>(ManagedChannelOrphanWrapper.java:93)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:53)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:44)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelImplBuilder.build(ManagedChannelImplBuilder.java:630)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:297)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:388)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.<init>(ChannelPool.java:105)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:83)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:236)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:230)\n",
      "\tat com.google.api.gax.rpc.ClientContext.create(ClientContext.java:201)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.GrpcTopicStatsServiceStub.create(GrpcTopicStatsServiceStub.java:93)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.TopicStatsServiceStubSettings.createStub(TopicStatsServiceStubSettings.java:120)\n",
      "\tat com.google.cloud.pubsublite.v1.TopicStatsServiceClient.<init>(TopicStatsServiceClient.java:144)\n",
      "\tat com.google.cloud.pubsublite.v1.TopicStatsServiceClient.create(TopicStatsServiceClient.java:125)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newTopicStatsServiceClient(PslReadDataSourceOptions.java:205)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newTopicStatsClient(PslReadDataSourceOptions.java:227)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newHeadOffsetReader(PslReadDataSourceOptions.java:237)\n",
      "\tat com.google.cloud.pubsublite.spark.PslMicroBatchStream.<init>(PslMicroBatchStream.java:57)\n",
      "\tat com.google.cloud.pubsublite.spark.PslScanBuilder.toMicroBatchStream(PslScanBuilder.java:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:173)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\n",
      "23/01/20 12:14:57 ERROR com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper: *~*~*~ Channel ManagedChannelImpl{logId=545, target=europe-central2-pubsublite.googleapis.com:443} was not shutdown properly!!! ~*~*~*\n",
      "    Make sure to call shutdown()/shutdownNow() and wait until awaitTermination() returns true.\n",
      "java.lang.RuntimeException: ManagedChannel allocation site\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference.<init>(ManagedChannelOrphanWrapper.java:93)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:53)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:44)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelImplBuilder.build(ManagedChannelImplBuilder.java:630)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:297)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:388)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.<init>(ChannelPool.java:105)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:83)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:236)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:230)\n",
      "\tat com.google.api.gax.rpc.ClientContext.create(ClientContext.java:201)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.GrpcCursorServiceStub.create(GrpcCursorServiceStub.java:96)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.CursorServiceStubSettings.createStub(CursorServiceStubSettings.java:199)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.<init>(CursorServiceClient.java:153)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.create(CursorServiceClient.java:134)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorServiceClient(PslReadDataSourceOptions.java:165)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorClient(PslReadDataSourceOptions.java:179)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newMultiPartitionCommitter(PslReadDataSourceOptions.java:125)\n",
      "\tat com.google.cloud.pubsublite.spark.PslMicroBatchStream.<init>(PslMicroBatchStream.java:56)\n",
      "\tat com.google.cloud.pubsublite.spark.PslScanBuilder.toMicroBatchStream(PslScanBuilder.java:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:173)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\n",
      "23/01/20 12:14:57 ERROR com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper: *~*~*~ Channel ManagedChannelImpl{logId=523, target=europe-central2-pubsublite.googleapis.com:443} was not shutdown properly!!! ~*~*~*\n",
      "    Make sure to call shutdown()/shutdownNow() and wait until awaitTermination() returns true.\n",
      "java.lang.RuntimeException: ManagedChannel allocation site\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference.<init>(ManagedChannelOrphanWrapper.java:93)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:53)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:44)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelImplBuilder.build(ManagedChannelImplBuilder.java:630)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:297)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:388)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.<init>(ChannelPool.java:105)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:83)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:236)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:230)\n",
      "\tat com.google.api.gax.rpc.ClientContext.create(ClientContext.java:201)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.GrpcCursorServiceStub.create(GrpcCursorServiceStub.java:96)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.CursorServiceStubSettings.createStub(CursorServiceStubSettings.java:199)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.<init>(CursorServiceClient.java:153)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.create(CursorServiceClient.java:134)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorServiceClient(PslReadDataSourceOptions.java:165)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorClient(PslReadDataSourceOptions.java:179)\n",
      "\tat com.google.cloud.pubsublite.spark.PslMicroBatchStream.<init>(PslMicroBatchStream.java:55)\n",
      "\tat com.google.cloud.pubsublite.spark.PslScanBuilder.toMicroBatchStream(PslScanBuilder.java:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:173)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\n",
      "23/01/20 12:14:57 ERROR com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper: *~*~*~ Channel ManagedChannelImpl{logId=561, target=europe-central2-pubsublite.googleapis.com:443} was not shutdown properly!!! ~*~*~*\n",
      "    Make sure to call shutdown()/shutdownNow() and wait until awaitTermination() returns true.\n",
      "java.lang.RuntimeException: ManagedChannel allocation site\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference.<init>(ManagedChannelOrphanWrapper.java:93)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:53)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:44)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelImplBuilder.build(ManagedChannelImplBuilder.java:630)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:297)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:388)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.<init>(ChannelPool.java:105)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:83)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:236)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:230)\n",
      "\tat com.google.api.gax.rpc.ClientContext.create(ClientContext.java:201)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.GrpcCursorServiceStub.create(GrpcCursorServiceStub.java:96)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.CursorServiceStubSettings.createStub(CursorServiceStubSettings.java:199)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.<init>(CursorServiceClient.java:153)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.create(CursorServiceClient.java:134)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorServiceClient(PslReadDataSourceOptions.java:165)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorClient(PslReadDataSourceOptions.java:179)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newMultiPartitionCommitter(PslReadDataSourceOptions.java:125)\n",
      "\tat com.google.cloud.pubsublite.spark.PslMicroBatchStream.<init>(PslMicroBatchStream.java:56)\n",
      "\tat com.google.cloud.pubsublite.spark.PslScanBuilder.toMicroBatchStream(PslScanBuilder.java:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:173)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\n",
      "23/01/20 12:14:57 ERROR com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper: *~*~*~ Channel ManagedChannelImpl{logId=547, target=europe-central2-pubsublite.googleapis.com:443} was not shutdown properly!!! ~*~*~*\n",
      "    Make sure to call shutdown()/shutdownNow() and wait until awaitTermination() returns true.\n",
      "java.lang.RuntimeException: ManagedChannel allocation site\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper$ManagedChannelReference.<init>(ManagedChannelOrphanWrapper.java:93)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:53)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelOrphanWrapper.<init>(ManagedChannelOrphanWrapper.java:44)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.ManagedChannelImplBuilder.build(ManagedChannelImplBuilder.java:630)\n",
      "\tat com.google.cloud.pubsublite.repackaged.io.grpc.internal.AbstractManagedChannelImplBuilder.build(AbstractManagedChannelImplBuilder.java:297)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createSingleChannel(InstantiatingGrpcChannelProvider.java:388)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.<init>(ChannelPool.java:105)\n",
      "\tat com.google.api.gax.grpc.ChannelPool.create(ChannelPool.java:83)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.createChannel(InstantiatingGrpcChannelProvider.java:236)\n",
      "\tat com.google.api.gax.grpc.InstantiatingGrpcChannelProvider.getTransportChannel(InstantiatingGrpcChannelProvider.java:230)\n",
      "\tat com.google.api.gax.rpc.ClientContext.create(ClientContext.java:201)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.GrpcCursorServiceStub.create(GrpcCursorServiceStub.java:96)\n",
      "\tat com.google.cloud.pubsublite.v1.stub.CursorServiceStubSettings.createStub(CursorServiceStubSettings.java:199)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.<init>(CursorServiceClient.java:153)\n",
      "\tat com.google.cloud.pubsublite.v1.CursorServiceClient.create(CursorServiceClient.java:134)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorServiceClient(PslReadDataSourceOptions.java:165)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newCursorClient(PslReadDataSourceOptions.java:179)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newMultiPartitionCommitter(PslReadDataSourceOptions.java:125)\n",
      "\tat com.google.cloud.pubsublite.spark.PslMicroBatchStream.<init>(PslMicroBatchStream.java:56)\n",
      "\tat com.google.cloud.pubsublite.spark.PslScanBuilder.toMicroBatchStream(PslScanBuilder.java:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:173)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\n",
      "23/01/20 12:14:57 ERROR org.apache.spark.sql.execution.streaming.MicroBatchExecution: Query [id = 9cfc7cf8-a086-41d6-a284-eb0046d1f5fb, runId = 7c49fb24-ec4b-43c1-afab-c086fe1d7cd8] terminated with error\n",
      "java.lang.IllegalStateException: Unable to fetch topic.\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.getTopicPath(PslReadDataSourceOptions.java:219)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.newHeadOffsetReader(PslReadDataSourceOptions.java:237)\n",
      "\tat com.google.cloud.pubsublite.spark.PslMicroBatchStream.<init>(PslMicroBatchStream.java:57)\n",
      "\tat com.google.cloud.pubsublite.spark.PslScanBuilder.toMicroBatchStream(PslScanBuilder.java:110)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:104)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:173)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:171)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:62)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:326)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\n",
      "Caused by: java.lang.InterruptedException\n",
      "\tat repackaged.com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:543)\n",
      "\tat com.google.cloud.pubsublite.spark.PslReadDataSourceOptions.getTopicPath(PslReadDataSourceOptions.java:217)\n",
      "\t... 23 more\n"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "    sdf.writeStream.format(\"console\")\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(processingTime=\"1 second\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# Wait 120 seconds (must be >= 60 seconds) to start receiving messages.\n",
    "# \n",
    "query.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
