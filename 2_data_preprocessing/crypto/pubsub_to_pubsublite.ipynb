{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d228de1d-498a-493e-8160-16cbc9452cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_number = 1072423212419\n",
    "project_id = \"crypto-busting-375023\"\n",
    "location = \"europe-central2\"\n",
    "subscription_id = \"bda-coinbase-topic-sub\"\n",
    "topic_id = \"bda-coinbase-topic\"\n",
    "timeout = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0387738c-8c63-4fb5-a533-20a2f1e7f150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import TimeoutError\n",
    "from google.cloud import pubsub_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dacf3082-1cae-4890-aea1-a78ecb1b3c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "subscriber = pubsub_v1.SubscriberClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "347c6f81-80b6-4b5d-a0b2-3524173ac535",
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_path = subscriber.subscription_path(project_id, subscription_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18a3fe13-56a9-4b48-8fdf-40211cf8b975",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afeeb2f6-6b6f-44e9-a7a3-e696f6aa8bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(message: pubsub_v1.subscriber.message.Message) -> None:\n",
    "    print(f\"Received {message}.\")\n",
    "    message.ack()\n",
    "    messages.append(message.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0125956-6609-4de7-b7b4-4c5d28ad3527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening for messages on projects/crypto-busting-375023/subscriptions/bda-coinbase-topic-sub..\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streaming_pull_future = subscriber.subscribe(subscription_path, callback=callback)\n",
    "print(f\"Listening for messages on {subscription_path}..\\n\")\n",
    "\n",
    "with subscriber:\n",
    "    try:\n",
    "        # When `timeout` is not set, result() will block indefinitely,\n",
    "        # unless an exception is encountered first.\n",
    "        streaming_pull_future.result(timeout=timeout)\n",
    "    except TimeoutError:\n",
    "        streaming_pull_future.cancel()  # Trigger the shutdown.\n",
    "        streaming_pull_future.result()  # Block until the shutdown is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66a4d95e-ee26-40cc-8ada-c4131ea4c74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d25534c3-7fa3-4f12-91e6-8ff25c97fd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mes in messages:\n",
    "    mes.decode('UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1bd7523-651c-42f6-bc05-03028624eb9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'side': '',\n",
       " 'price': '1549.4',\n",
       " 'product_id': 'ETH-USD',\n",
       " 'time': '2023-01-20T12:04:14.537578Z'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = [ast.literal_eval(mes.decode(\"UTF-8\")) for mes in messages]\n",
    "response[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c60a4c-cacc-4ef1-b787-4a34361428f1",
   "metadata": {},
   "source": [
    "# Pub/Sub Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "036bbabd-4bdc-4387-891a-eb62ce93c202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-pubsub==2.10 in /opt/conda/miniconda3/lib/python3.8/site-packages (2.10.0)\n",
      "Requirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.28.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-cloud-pubsub==2.10) (1.34.0)\n",
      "Requirement already satisfied: grpcio-status>=1.16.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-cloud-pubsub==2.10) (1.48.2)\n",
      "Requirement already satisfied: proto-plus>=1.7.1 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-cloud-pubsub==2.10) (1.22.2)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.38.1 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-cloud-pubsub==2.10) (1.51.1)\n",
      "Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-cloud-pubsub==2.10) (0.12.4)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.28.0->google-cloud-pubsub==2.10) (2.25.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.28.0->google-cloud-pubsub==2.10) (1.57.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.28.0->google-cloud-pubsub==2.10) (1.35.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.28.0->google-cloud-pubsub==2.10) (3.20.3)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.28.0->google-cloud-pubsub==2.10) (59.8.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.28.0->google-cloud-pubsub==2.10) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.28.0->google-cloud-pubsub==2.10) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.28.0->google-cloud-pubsub==2.10) (0.2.7)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.28.0->google-cloud-pubsub==2.10) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.28.0->google-cloud-pubsub==2.10) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.28.0->google-cloud-pubsub==2.10) (2022.12.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.28.0->google-cloud-pubsub==2.10) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.28.0->google-cloud-pubsub==2.10) (4.0.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/miniconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.28.0->google-cloud-pubsub==2.10) (0.4.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install google-cloud-pubsub==2.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03bfbbe8-0826-4566-bedf-1a80d859266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud.pubsublite.cloudpubsub import PublisherClient\n",
    "from google.cloud.pubsublite.types import (\n",
    "    CloudRegion,\n",
    "    CloudZone,\n",
    "    MessageMetadata,\n",
    "    TopicPath,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a99d2fa-2fb7-4859-8996-7f79779fc832",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_id = \"bda-coinbase-topic-lite\"\n",
    "subscription_id = \"bda-coinbase-sub-lite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75d71853-7ade-462d-b0ba-8d38791a88ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = CloudRegion(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b23f108f-b347-43ed-839d-34a2749dee0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CloudRegion(name='europe-central2')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc.region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "192a37c1-c036-4671-a722-2c30c00eb210",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_path = TopicPath(project_number, loc, topic_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "439fa647-545a-4260-b47f-3dbfbf9c4968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published a message to projects/1072423212419/locations/europe-central2/topics/bda-coinbase-topic-lite with partition 0 and offset 10.\n",
      "Published a message to projects/1072423212419/locations/europe-central2/topics/bda-coinbase-topic-lite with partition 0 and offset 11.\n",
      "Published a message to projects/1072423212419/locations/europe-central2/topics/bda-coinbase-topic-lite with partition 0 and offset 12.\n",
      "Published a message to projects/1072423212419/locations/europe-central2/topics/bda-coinbase-topic-lite with partition 0 and offset 13.\n",
      "Published a message to projects/1072423212419/locations/europe-central2/topics/bda-coinbase-topic-lite with partition 0 and offset 14.\n",
      "Published a message to projects/1072423212419/locations/europe-central2/topics/bda-coinbase-topic-lite with partition 0 and offset 15.\n",
      "Published a message to projects/1072423212419/locations/europe-central2/topics/bda-coinbase-topic-lite with partition 0 and offset 16.\n",
      "Published a message to projects/1072423212419/locations/europe-central2/topics/bda-coinbase-topic-lite with partition 0 and offset 17.\n",
      "Published a message to projects/1072423212419/locations/europe-central2/topics/bda-coinbase-topic-lite with partition 0 and offset 18.\n",
      "Published a message to projects/1072423212419/locations/europe-central2/topics/bda-coinbase-topic-lite with partition 0 and offset 19.\n",
      "Published a message to projects/1072423212419/locations/europe-central2/topics/bda-coinbase-topic-lite with partition 0 and offset 20.\n",
      "Published a message to projects/1072423212419/locations/europe-central2/topics/bda-coinbase-topic-lite with partition 0 and offset 21.\n",
      "Published a message to projects/1072423212419/locations/europe-central2/topics/bda-coinbase-topic-lite with partition 0 and offset 22.\n",
      "Published a message to projects/1072423212419/locations/europe-central2/topics/bda-coinbase-topic-lite with partition 0 and offset 23.\n",
      "Published a message to projects/1072423212419/locations/europe-central2/topics/bda-coinbase-topic-lite with partition 0 and offset 24.\n"
     ]
    }
   ],
   "source": [
    "with PublisherClient() as publisher_client:\n",
    "    for msg in messages:\n",
    "        api_future = publisher_client.publish(topic_path, msg)\n",
    "        # result() blocks. To resolve API futures asynchronously, use add_done_callback().\n",
    "        message_id = api_future.result()\n",
    "        message_metadata = MessageMetadata.decode(message_id)\n",
    "        print(\n",
    "            f\"Published a message to {topic_path} with partition {message_metadata.partition.value} and offset {message_metadata.cursor.offset}.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfe3ae2-2467-4a50-a702-b77e9f14e083",
   "metadata": {},
   "source": [
    "# Pub/Sub Lite to SparkStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9351a9e6-cffa-42ae-bd1b-5223a99e7af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8ee43e7-a7ff-4b44-83c4-d415d34dffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_number = 1072423212419\n",
    "location = \"europe-central2\"\n",
    "lite_subscription_id = \"bda-coinbase-sub-lite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "326bcb98-b1eb-4dab-a67e-ddc705101eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials_path = \"/home/bda_crypto_busters/keys/cloud_compute_key.json\"\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = credentials_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b992ef3-fd87-4f1d-a19b-8a96f85213ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9bfae20d-cd97-49fa-aeee-6753b6c4cfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Read Pub/Sub Stream\").master(\"yarn\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "302b5f7a-96d1-4520-87ca-b3cc345c1338",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/21 14:58:38 WARN org.apache.spark.SparkContext: The jar gs://spark-lib/pubsublite/pubsublite-spark-sql-streaming-LATEST-with-dependencies.jar has been added already. Overwriting of added jars is not supported in the current version.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[result: int]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.sql('add jar file:////home/bda_reddit/dependancies/spark-streaming-pubsub_2.11.jar')\n",
    "# spark.sql('add jar file:////home/bda_reddit/dependancies/spark-streaming_2.13-3.3.1.jar')\n",
    "# spark.sql('add jar file:////home/bda_reddit/repos/BigDataAnalytics/2_data_preprocessing/pubsublite-spark-sql-streaming-0.4.2.jar')\n",
    "# spark.sql('add jar file:////home/bda_reddit/dependancies/spark-catalyst_2.13-3.3.1.jar')\n",
    "spark.sql(\"add jar gs://spark-lib/pubsublite/pubsublite-spark-sql-streaming-LATEST-with-dependencies.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be5620a6-a470-4f28-838b-8fffd108e407",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"add jar file:////home/bda_reddit/dependancies/gax-grpc-1.53.1.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf3b4db7-f072-425d-b448-3db0caf17902",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = (\n",
    "    spark.readStream.format(\"pubsublite\")\n",
    "    .option(\n",
    "        \"pubsublite.subscription\",\n",
    "        f\"projects/{project_number}/locations/{location}/subscriptions/{lite_subscription_id}\",\n",
    "    )\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82e5272e-ded4-4865-b1ea-dbd5ed3768f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- subscription: string (nullable = false)\n",
      " |-- partition: long (nullable = false)\n",
      " |-- offset: long (nullable = false)\n",
      " |-- key: binary (nullable = false)\n",
      " |-- data: string (nullable = false)\n",
      " |-- publish_timestamp: timestamp (nullable = false)\n",
      " |-- event_timestamp: timestamp (nullable = true)\n",
      " |-- attributes: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: array (valueContainsNull = true)\n",
      " |    |    |-- element: binary (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf.withColumn(\"data\", sdf.data.cast(StringType()))\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "823afdb3-583e-45ed-be64-6b514adb9697",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/21 14:58:50 WARN org.apache.spark.sql.streaming.StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-79dd53f0-a78f-495b-a190-f7ab2d1cac5c. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/01/21 14:58:50 WARN org.apache.spark.sql.streaming.StreamingQueryManager: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/01/21 14:58:52 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 2.0 (TID 8) (bda-hdfs-w-0.europe-central2-a.c.crypto-busting-375023.internal executor 2): org.apache.spark.util.TaskCompletionListenerException: \n",
      "\n",
      "Previous exception in task: Failed to retrieve messages.\n",
      "\tcom.google.cloud.pubsublite.spark.PslMicroBatchInputPartitionReader.next(PslMicroBatchInputPartitionReader.java:74)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:79)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:112)\n",
      "\torg.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tscala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\torg.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\torg.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\torg.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:413)\n",
      "\torg.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:452)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:360)\n",
      "\torg.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\torg.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\torg.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n",
      "\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\torg.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n",
      "\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tjava.lang.Thread.run(Thread.java:750)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:145)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "23/01/21 14:58:53 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 2.0 failed 4 times; aborting job\n",
      "23/01/21 14:58:53 ERROR org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@757890e3 is aborting.\n",
      "23/01/21 14:58:53 ERROR org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@757890e3 aborted.\n",
      "23/01/21 14:58:53 ERROR org.apache.spark.sql.execution.streaming.MicroBatchExecution: Query [id = c72566d2-da3a-41ea-8711-be25d029fd76, runId = b1d52c23-05a1-43bb-a0b2-6895d5143940] terminated with error\n",
      "org.apache.spark.SparkException: Writing job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:336)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:297)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:40)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:40)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:46)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3709)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2978)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3700)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3698)\n",
      "\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:2978)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:589)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$15(MicroBatchExecution.scala:584)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:357)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:355)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:584)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:226)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:357)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:355)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:194)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:334)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:317)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 11) (bda-hdfs-w-0.europe-central2-a.c.crypto-busting-375023.internal executor 2): org.apache.spark.util.TaskCompletionListenerException: \n",
      "\n",
      "Previous exception in task: Failed to retrieve messages.\n",
      "\tcom.google.cloud.pubsublite.spark.PslMicroBatchInputPartitionReader.next(PslMicroBatchInputPartitionReader.java:74)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:79)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:112)\n",
      "\torg.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tscala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\torg.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\torg.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\torg.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:413)\n",
      "\torg.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:452)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:360)\n",
      "\torg.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\torg.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\torg.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n",
      "\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\torg.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n",
      "\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tjava.lang.Thread.run(Thread.java:750)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:145)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2204)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:357)\n",
      "\t... 40 more\n"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "        sdf.writeStream.format(\"console\")\n",
    "        .outputMode(\"append\")\n",
    "        .trigger(processingTime=\"1 second\")\n",
    "        .start()\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
