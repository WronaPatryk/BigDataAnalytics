{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7da86073-77b2-4e05-af6c-b8282a6ff6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ac40506-ab47-460b-b382-8c114e78ed10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67eb69c2-4d8b-45e5-b4a0-7fd13145b6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_file = open(\"/home/bda_crypto_busters/repos/BigDataAnalytics/2_data_preprocessing/crypto/stream/crypto-busting-375023-6722d6967eca.json\", \"rb\")\n",
    "key = base64.b64encode(key_file.read())\n",
    "key = key.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afeef34e-dc98-49e9-97fe-9b2f19185567",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/01/27 23:03:51 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "23/01/27 23:03:51 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "23/01/27 23:03:52 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/01/27 23:03:52 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"Spark Streaming to Pub/Sub Lite\") \\\n",
    "    .config(\"spark.jars\", \"/home/bda_crypto_busters/repos/BigDataAnalytics/2_data_preprocessing/crypto/stream/pubsublite-spark-sql-streaming-LATEST-with-dependencies.jar\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3cc8a14-8364-46ce-accd-a31c391763c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_number = 1072423212419\n",
    "location = \"europe-central2\"\n",
    "subscription_id = \"bda-coinbase-sub-lite\"\n",
    "df = (\n",
    "    spark.readStream.format(\"pubsublite\")\n",
    "    .option(\n",
    "        \"pubsublite.subscription\",\n",
    "        f\"projects/{project_number}/locations/{location}/subscriptions/{subscription_id}\",\n",
    "    )\n",
    "    .option(\"gcp.credentials.key\", key)\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a36106e6-7263-4aad-9257-34f942a761dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w tej kolumnie powinny być dane ale są zakodowane jako json?\n",
    "df = df.withColumn('data', df.data.cast(StringType())).select('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd57191d-cd55-4460-b849-fdec0121feb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, to_json\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ae49de8-cc7d-4249-bc10-77f3efe4dd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSONschema = StructType([ \n",
    "    StructField(\"side\", StringType(), nullable=True), \n",
    "    StructField(\"price\", StringType(), nullable=False), \n",
    "    StructField(\"product_id\", StringType(), nullable=False),\n",
    "    StructField(\"time\", TimestampType(), nullable=False), \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a15aa8b1-273a-4fb0-a4ed-4d1ae034a0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = df.withColumn(\"JSONData\", from_json(col(\"data\"), JSONschema)).select(\"JSONData.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66bfd8cf-1e4c-4721-9675-74f685893a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.withColumn(\"price\", sdf.price.cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5db5bc9-0faf-4619-b846-9dbdb249b318",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_price = sdf.select(\"price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9d5a391-c8d4-4fc4-8ac8-6020bf79b0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e6743db-e211-4e8e-b034-52dae97be2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- side: string (nullable = true)\n",
      " |-- price: float (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d352e17c-85ed-4cbf-bc55-99f7ff06160d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efbf3bd7-9de1-46b1-8832-4c27107b6232",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import array, create_map, col, lit, when, struct\n",
    "from pyspark.sql.types import BinaryType, StringType\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28b02ed2-0644-43d1-b961-4e95115483bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = (\n",
    "    sdf.withColumn(\"key\", col(\"time\").cast(StringType()).cast(BinaryType()))\n",
    "    .withColumn(\"data\", to_json(struct(\"product_id\", \"price\")).cast(BinaryType()))\n",
    "    .withColumnRenamed(\"time\", \"event_timestamp\")\n",
    "    .withColumn(\n",
    "        \"attributes\",\n",
    "        create_map(\n",
    "            lit(\"key1\"),\n",
    "            array(when(col(\"price\") > 100, b\"huge\").otherwise(b\"low\")),\n",
    "        ),\n",
    "    )\n",
    "    .drop(\"side\", \"product_id\", \"price\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a77e346-e495-4fa4-8b80-d2fc4b43cb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_timestamp: timestamp (nullable = true)\n",
      " |-- key: binary (nullable = true)\n",
      " |-- data: binary (nullable = true)\n",
      " |-- attributes: map (nullable = false)\n",
      " |    |-- key: string\n",
      " |    |-- value: array (valueContainsNull = false)\n",
      " |    |    |-- element: binary (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "message.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e1d379e-5ca3-4319-816f-9d51557fa171",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_topic_id = \"spark-coinbase-lite-topic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "945c45ff-29e5-44d9-a51f-5c17899c75ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/27 23:09:39 WARN org.apache.spark.sql.streaming.StreamingQueryManager: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/01/27 23:09:48 WARN org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 7865 milliseconds\n",
      "23/01/27 23:12:46 WARN org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 2030 milliseconds\n",
      "23/01/27 23:13:46 WARN org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 1730 milliseconds\n",
      "23/01/27 23:17:50 WARN org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 1413 milliseconds\n",
      "23/01/27 23:18:51 WARN org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 1818 milliseconds\n",
      "23/01/27 23:22:55 WARN org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 1436 milliseconds\n"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "    message.writeStream.format(\"pubsublite\")\n",
    "    .option(\n",
    "        \"pubsublite.topic\",\n",
    "        f\"projects/{project_number}/locations/{location}/topics/{save_topic_id}\",\n",
    "    )\n",
    "    .option(\"gcp.credentials.key\", key)\n",
    "    # Required. Use a unique checkpoint location for each job.\n",
    "    .option(\"checkpointLocation\", \"/tmp/app\" + uuid.uuid4().hex)\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(processingTime=\"1 second\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "133137c7-edc3-4665-8526-b0db2dde23a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
