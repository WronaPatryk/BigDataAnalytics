{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7da86073-77b2-4e05-af6c-b8282a6ff6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ac40506-ab47-460b-b382-8c114e78ed10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67eb69c2-4d8b-45e5-b4a0-7fd13145b6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_file = open(\"/home/bda_crypto_busters/repos/BigDataAnalytics/2_data_preprocessing/crypto/stream/crypto-busting-375023-6722d6967eca.json\", \"rb\")\n",
    "key = base64.b64encode(key_file.read())\n",
    "key = key.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afeef34e-dc98-49e9-97fe-9b2f19185567",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/01/22 23:55:24 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "23/01/22 23:55:24 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "23/01/22 23:55:24 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/01/22 23:55:25 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"Spark Streaming to Pub/Sub Lite\") \\\n",
    "    .config(\"spark.jars\", \"/home/bda_crypto_busters/repos/BigDataAnalytics/2_data_preprocessing/crypto/stream/pubsublite-spark-sql-streaming-LATEST-with-dependencies.jar\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3cc8a14-8364-46ce-accd-a31c391763c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_number = 1072423212419\n",
    "location = \"europe-central2\"\n",
    "subscription_id = \"bda-coinbase-sub-lite\"\n",
    "df = (\n",
    "    spark.readStream.format(\"pubsublite\")\n",
    "    .option(\n",
    "        \"pubsublite.subscription\",\n",
    "        f\"projects/{project_number}/locations/{location}/subscriptions/{subscription_id}\",\n",
    "    )\n",
    "    .option(\"gcp.credentials.key\", key)\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a36106e6-7263-4aad-9257-34f942a761dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w tej kolumnie powinny być dane ale są zakodowane jako json?\n",
    "df = df.withColumn('data', df.data.cast(StringType())).select('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6af46440-c3f2-4bb6-b7d8-382a155f6595",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/22 23:56:04 WARN org.apache.spark.sql.streaming.StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-06891958-0442-4eed-b3a5-ab605f7bc899. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/01/22 23:56:04 WARN org.apache.spark.sql.streaming.StreamingQueryManager: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "# Print\n",
    "res = df.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd57191d-cd55-4460-b849-fdec0121feb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, to_json\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ae49de8-cc7d-4249-bc10-77f3efe4dd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSONschema = StructType([ \n",
    "    StructField(\"side\", StringType(), True), \n",
    "    StructField(\"price\", StringType(), False), \n",
    "    StructField(\"product_id\", StringType(), False),\n",
    "    StructField(\"time\", StringType(), False), \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a15aa8b1-273a-4fb0-a4ed-4d1ae034a0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = df.withColumn(\"JSONData\", from_json(col(\"data\"), JSONschema)).select(\"JSONData.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66bfd8cf-1e4c-4721-9675-74f685893a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.withColumn(\"price\", sdf.price.cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9d5a391-c8d4-4fc4-8ac8-6020bf79b0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e6743db-e211-4e8e-b034-52dae97be2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- side: string (nullable = true)\n",
      " |-- price: float (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efbf3bd7-9de1-46b1-8832-4c27107b6232",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import array, create_map, col, lit, when, struct\n",
    "from pyspark.sql.types import BinaryType, StringType\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b02ed2-0644-43d1-b961-4e95115483bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = (\n",
    "    sdf.withColumn(\"key\", col(\"time\").cast(BinaryType()))\n",
    "    .withColumn(\"data\", to_json())\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
