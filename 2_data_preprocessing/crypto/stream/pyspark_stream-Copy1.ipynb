{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d499e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StringType\n",
    "from pyspark.ml.regression import LinearRegression, LinearRegressionModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.functions import split, col, when, mean, isnan, lit\n",
    "import base64\n",
    "import os\n",
    "\n",
    "# Temporary\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d97f6f28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "key_file = open(\"/home/bda_crypto_busters/repos/BigDataAnalytics/2_data_preprocessing/crypto/stream/crypto-busting-375023-6722d6967eca.json\", \"rb\")\n",
    "key = base64.b64encode(key_file.read())\n",
    "key = key.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5e3cc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/01/21 17:18:51 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "23/01/21 17:18:51 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "23/01/21 17:18:52 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/01/21 17:18:52 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"Pub/Sub Lite to Spark Streaming\") \\\n",
    "    .config(\"spark.jars\", \"/home/bda_crypto_busters/repos/BigDataAnalytics/2_data_preprocessing/crypto/stream/pubsublite-spark-sql-streaming-LATEST-with-dependencies.jar\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d634804",
   "metadata": {},
   "source": [
    "### Reading streaming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b56fe1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_number = 1072423212419\n",
    "location = \"europe-central2\"\n",
    "subscription_id = \"bda-coinbase-sub-lite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d9ccb37-a078-4151-9f60-54099945b502",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.readStream.format(\"pubsublite\")\n",
    "    .option(\n",
    "        \"pubsublite.subscription\",\n",
    "        f\"projects/{project_number}/locations/{location}/subscriptions/{subscription_id}\",\n",
    "    )\n",
    "    .option(\"gcp.credentials.key\", key)\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e58fd131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w tej kolumnie powinny byÄ‡ dane\n",
    "# df = df.withColumn('data', df.data.cast(StringType())).select('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81b7dca4-3c5d-416c-b290-87497f869454",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('data', df.data.cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2303bf11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/21 17:19:25 WARN org.apache.spark.sql.streaming.StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-4dc01046-9aa9-4a66-9bd9-4eb2b951e3f2. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/01/21 17:19:25 WARN org.apache.spark.sql.streaming.StreamingQueryManager: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/01/21 17:19:48 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/01/21 17:20:03 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/01/21 17:20:18 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/01/21 17:20:33 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/01/21 17:20:48 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
     ]
    }
   ],
   "source": [
    "# Print\n",
    "res = df.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e40f02c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
