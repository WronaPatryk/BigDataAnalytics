{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f900bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec0a67e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Text: string (nullable = true)\n",
      " |-- Sentiment: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"sentiment_data\", header = True, inferSchema = True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0edc351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6853"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6df6d2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Text: string (nullable = true)\n",
      " |-- Sentiment: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.na.drop()\n",
    "df = df.na.replace(-1, 0)\n",
    "df = df.withColumn(\"Sentiment\", df.Sentiment.cast('double'))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6b34b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|                Text|Sentiment|\n",
      "+--------------------+---------+\n",
      "|should be in ever...|      1.0|\n",
      "|No one convinced ...|      0.0|\n",
      "|I think it can be...|      1.0|\n",
      "|Bear market is ov...|      1.0|\n",
      "|I posted this bef...|      1.0|\n",
      "|The value of WAY ...|      0.0|\n",
      "|I t imagine being...|      0.0|\n",
      "|Not trust this ho...|      0.0|\n",
      "|is Hope Love Many...|      1.0|\n",
      "|We can no longer ...|      1.0|\n",
      "|  In short invest in|      1.0|\n",
      "|I do not feel sor...|      0.0|\n",
      "|is pretty stable ...|      1.0|\n",
      "|I don t even know...|      0.0|\n",
      "|Funny how people ...|      0.0|\n",
      "|If you lose money...|      0.0|\n",
      "|        buy more sol|      1.0|\n",
      "|good time to accu...|      1.0|\n",
      "|is plain and simp...|      1.0|\n",
      "|A great way to br...|      1.0|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "397a1876",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67773c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/17 15:52:29 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/12/17 15:52:29 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"Text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001, labelCol='Sentiment')\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "487adbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Text: string (nullable = true)\n",
      " |-- Sentiment: double (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test documents and print columns of interest.\n",
    "prediction = model.transform(test)\n",
    "prediction.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7627f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7711901435028371"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval = BinaryClassificationEvaluator(labelCol=\"Sentiment\")\n",
    "eval.evaluate(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90738688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist_reddit_schema = StructType(\n",
    "#    [StructField('subreddit', StringType(), True),\n",
    "#     StructField('title', StringType(), True),\n",
    "#     StructField('selftext', StringType(), True),\n",
    "#     StructField('created_utc', TimestampType(), True),\n",
    "#    ]\n",
    "#   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "947f758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_red = spark.read.csv(\"historical_reddit\", header = True, inferSchema = True)\n",
    "# df_red.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25d4845f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------+----+---+---+\n",
      "|11/04/2020|0.9576058280146804|solana|2020| 04| 11|\n",
      "+----------+------------------+------+----+---+---+\n",
      "|12/04/2020|0.7847113148208426|solana|2020|  4| 12|\n",
      "|13/04/2020|0.8759944068917709|solana|2020|  4| 13|\n",
      "|14/04/2020|0.7867121945458646|solana|2020|  4| 14|\n",
      "|15/04/2020| 0.666673390515131|solana|2020|  4| 15|\n",
      "|16/04/2020|0.6376210673084666|solana|2020|  4| 16|\n",
      "|17/04/2020|0.6923331250859114|solana|2020|  4| 17|\n",
      "|18/04/2020| 0.657449398309544|solana|2020|  4| 18|\n",
      "|19/04/2020|0.6769721852162044|solana|2020|  4| 19|\n",
      "|20/04/2020|0.6094358389483635|solana|2020|  4| 20|\n",
      "|21/04/2020|0.5347904942320182|solana|2020|  4| 21|\n",
      "|22/04/2020|0.5728269210770057|solana|2020|  4| 22|\n",
      "|23/04/2020|0.6848872198147996|solana|2020|  4| 23|\n",
      "|24/04/2020|0.6217032788666345|solana|2020|  4| 24|\n",
      "|25/04/2020|0.6227650900853694|solana|2020|  4| 25|\n",
      "|26/04/2020|0.6470903181859691|solana|2020|  4| 26|\n",
      "|27/04/2020|0.6398628040694914|solana|2020|  4| 27|\n",
      "|28/04/2020|0.5683154806973284|solana|2020|  4| 28|\n",
      "|29/04/2020|0.6574283212299112|solana|2020|  4| 29|\n",
      "|30/04/2020|0.7411890135877649|solana|2020|  4| 30|\n",
      "|01/05/2020| 0.693089721323073|solana|2020|  5|  1|\n",
      "+----------+------------------+------+----+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df_red.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2486ebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prosty model regresji na szeregów czasowych najlpiej na historical \n",
    "# Testy na historical dla MSE\n",
    "# Predykcja powinna opierac się na cenach z ostanich 24h + na obecnym / przeszłym sentymencie z reddita.\n",
    "# Na razie tylko na podstawie crypto* na nast. godzinę\n",
    "# Zapisywać i wczytać te modele jakoś?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0cc251",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cryp = spark.read.csv(\"historical_crypto\", header = True, inferSchema = True)\n",
    "df_cryp.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
